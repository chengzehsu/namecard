"""
æœ€ä½³åŒ– AI æœå‹™æ•ˆèƒ½æ¸¬è©¦ - AI Engineer å¯¦ä½œ  
æ¸¬è©¦ç”Ÿç”¢ç’°å¢ƒä¸‹çš„ AI è™•ç†ç®¡ç·šæ•ˆèƒ½
"""

import asyncio
import time
import pytest
import statistics
from typing import List, Dict, Any
from unittest.mock import AsyncMock, MagicMock, patch
import sys
import os

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from src.namecard.infrastructure.ai.optimized_ai_service import (
    create_optimized_ai_service,
    ProcessingPriority
)


class TestOptimizedAIPerformance:
    """æœ€ä½³åŒ– AI æœå‹™æ•ˆèƒ½æ¸¬è©¦å¥—ä»¶"""

    @pytest.fixture
    async def ai_service(self):
        """æä¾›æ¸¬è©¦ç”¨çš„ AI æœå‹™"""
        # æ¨¡æ“¬ç’°å¢ƒè®Šæ•¸
        os.environ['GOOGLE_API_KEY'] = 'test_key_1'
        os.environ['GOOGLE_API_KEY_FALLBACK'] = 'test_key_2'
        
        service = await create_optimized_ai_service(
            max_concurrent=5,
            cache_memory_mb=50,
            cache_disk_mb=100,
            auto_start=True
        )
        
        yield service
        
        await service.stop_service()

    @pytest.fixture
    def mock_image_data(self):
        """æä¾›æ¸¬è©¦ç”¨çš„åœ–ç‰‡è³‡æ–™"""
        return [
            b"fake_image_data_1" + b"x" * 1000,  # 1KB
            b"fake_image_data_2" + b"x" * 5000,  # 5KB  
            b"fake_image_data_3" + b"x" * 10000, # 10KB
        ]

    @pytest.fixture
    def mock_ai_response(self):
        """æ¨¡æ“¬ AI å›æ‡‰"""
        return {
            "card_count": 1,
            "cards": [{
                "card_index": 1,
                "confidence_score": 0.85,
                "name": "æ¸¬è©¦ç”¨æˆ¶",
                "company": "æ¸¬è©¦å…¬å¸",
                "email": "test@example.com",
                "phone": "+886912345678"
            }],
            "overall_quality": "good"
        }

    @pytest.mark.asyncio
    async def test_concurrent_request_performance(self, ai_service, mock_image_data, mock_ai_response):
        """æ¸¬è©¦ä½µç™¼è«‹æ±‚æ•ˆèƒ½"""
        
        # æ¨¡æ“¬ Gemini API å‘¼å«
        with patch('google.generativeai.GenerativeModel') as mock_model:
            mock_instance = mock_model.return_value
            mock_response = MagicMock()
            mock_response.text = '{"card_count": 1, "cards": [{"name": "Test"}], "overall_quality": "good"}'
            mock_instance.generate_content.return_value = mock_response
            
            # ä½µç™¼è«‹æ±‚æ¸¬è©¦
            concurrent_requests = 20
            start_time = time.time()
            
            tasks = []
            for i in range(concurrent_requests):
                task = ai_service.process_image(
                    image_bytes=mock_image_data[i % len(mock_image_data)],
                    priority=ProcessingPriority.NORMAL,
                    user_id=f"test_user_{i}"
                )
                tasks.append(task)
            
            # åŸ·è¡Œæ‰€æœ‰ä½µç™¼è«‹æ±‚
            results = await asyncio.gather(*tasks, return_exceptions=True)
            end_time = time.time()
            
            # åˆ†æçµæœ
            successful_results = [r for r in results if not isinstance(r, Exception)]
            failed_results = [r for r in results if isinstance(r, Exception)]
            
            total_time = end_time - start_time
            avg_time_per_request = total_time / concurrent_requests
            throughput = concurrent_requests / total_time
            
            print(f"\nğŸš€ ä½µç™¼æ•ˆèƒ½æ¸¬è©¦çµæœ:")
            print(f"   - ä½µç™¼è«‹æ±‚æ•¸: {concurrent_requests}")
            print(f"   - ç¸½åŸ·è¡Œæ™‚é–“: {total_time:.2f}s")
            print(f"   - å¹³å‡æ¯è«‹æ±‚æ™‚é–“: {avg_time_per_request:.2f}s")
            print(f"   - ååé‡: {throughput:.2f} requests/sec")
            print(f"   - æˆåŠŸç‡: {len(successful_results)}/{concurrent_requests}")
            
            # æ•ˆèƒ½åŸºæº–æ¸¬è©¦
            assert len(successful_results) >= concurrent_requests * 0.95, "æˆåŠŸç‡ä½æ–¼95%"
            assert total_time < 30, f"ç¸½åŸ·è¡Œæ™‚é–“éé•·: {total_time:.2f}s"
            assert throughput > 1.0, f"ååé‡éä½: {throughput:.2f} req/s"

    @pytest.mark.asyncio  
    async def test_cache_performance_impact(self, ai_service, mock_image_data):
        """æ¸¬è©¦å¿«å–å°æ•ˆèƒ½çš„å½±éŸ¿"""
        
        with patch('google.generativeai.GenerativeModel') as mock_model:
            mock_instance = mock_model.return_value
            mock_response = MagicMock()
            mock_response.text = '{"card_count": 1, "cards": [{"name": "Test"}], "overall_quality": "good"}'
            mock_instance.generate_content.return_value = mock_response
            
            test_image = mock_image_data[0]
            
            # ç¬¬ä¸€æ¬¡è«‹æ±‚ (æ‡‰è©²æœƒå‘¼å« API)
            start_time = time.time()  
            result1, metadata1 = await ai_service.process_image(
                image_bytes=test_image,
                enable_cache=True,
                user_id="cache_test_user"
            )
            first_request_time = time.time() - start_time
            
            # ç¬¬äºŒæ¬¡è«‹æ±‚ç›¸åŒåœ–ç‰‡ (æ‡‰è©²å‘½ä¸­å¿«å–)
            start_time = time.time()
            result2, metadata2 = await ai_service.process_image(  
                image_bytes=test_image,
                enable_cache=True,
                user_id="cache_test_user"
            )
            second_request_time = time.time() - start_time
            
            print(f"\nâš¡ å¿«å–æ•ˆèƒ½æ¸¬è©¦çµæœ:")
            print(f"   - ç¬¬ä¸€æ¬¡è«‹æ±‚æ™‚é–“: {first_request_time:.3f}s (å¿«å–: {metadata1['cache_hit']})")
            print(f"   - ç¬¬äºŒæ¬¡è«‹æ±‚æ™‚é–“: {second_request_time:.3f}s (å¿«å–: {metadata2['cache_hit']})")
            print(f"   - æ•ˆèƒ½æå‡: {(first_request_time / second_request_time):.1f}x")
            
            # é©—è­‰å¿«å–è¡Œç‚º
            assert not metadata1['cache_hit'], "ç¬¬ä¸€æ¬¡è«‹æ±‚ä¸æ‡‰è©²å‘½ä¸­å¿«å–"
            assert metadata2['cache_hit'], "ç¬¬äºŒæ¬¡è«‹æ±‚æ‡‰è©²å‘½ä¸­å¿«å–"
            assert second_request_time < first_request_time * 0.1, "å¿«å–è«‹æ±‚æ‡‰è©²é¡¯è‘—æ›´å¿«"

    @pytest.mark.asyncio
    async def test_batch_processing_efficiency(self, ai_service, mock_image_data):
        """æ¸¬è©¦æ‰¹æ¬¡è™•ç†æ•ˆç‡"""
        
        with patch('google.generativeai.GenerativeModel') as mock_model:
            mock_instance = mock_model.return_value
            mock_response = MagicMock()
            mock_response.text = '{"card_count": 1, "cards": [{"name": "Test"}], "overall_quality": "good"}'
            mock_instance.generate_content.return_value = mock_response
            
            # å»ºç«‹æ‰¹æ¬¡æ¸¬è©¦è³‡æ–™
            batch_size = 10
            batch_images = mock_image_data * (batch_size // len(mock_image_data) + 1)
            batch_images = batch_images[:batch_size]
            
            # æ‰¹æ¬¡è™•ç†æ¸¬è©¦
            start_time = time.time()
            results, batch_metadata = await ai_service.process_batch(
                image_batch=batch_images,
                max_concurrent=5,
                user_id="batch_test_user"
            )
            batch_time = time.time() - start_time
            
            # å–®ç¨è™•ç†æ¸¬è©¦ï¼ˆæ¯”è¼ƒåŸºæº–ï¼‰
            start_time = time.time()
            individual_results = []
            for image in batch_images:
                result, _ = await ai_service.process_image(
                    image_bytes=image,
                    user_id="individual_test_user"
                )
                individual_results.append(result)
            individual_time = time.time() - start_time
            
            efficiency_gain = individual_time / batch_time
            
            print(f"\nğŸ“¦ æ‰¹æ¬¡è™•ç†æ•ˆç‡æ¸¬è©¦:")
            print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
            print(f"   - æ‰¹æ¬¡è™•ç†æ™‚é–“: {batch_time:.2f}s")
            print(f"   - å€‹åˆ¥è™•ç†æ™‚é–“: {individual_time:.2f}s")
            print(f"   - æ•ˆç‡æå‡: {efficiency_gain:.1f}x")
            print(f"   - æ‰¹æ¬¡æˆåŠŸç‡: {batch_metadata['processing_summary']['successful']}/{batch_size}")
            
            # é©—è­‰æ‰¹æ¬¡è™•ç†æ•ˆç‡
            assert len(results) == batch_size, "æ‰¹æ¬¡çµæœæ•¸é‡ä¸æ­£ç¢º"
            assert efficiency_gain > 1.5, f"æ‰¹æ¬¡è™•ç†æ•ˆç‡æå‡ä¸è¶³: {efficiency_gain:.1f}x"
            assert batch_metadata['processing_summary']['successful'] >= batch_size * 0.9, "æ‰¹æ¬¡æˆåŠŸç‡ä½æ–¼90%"

    @pytest.mark.asyncio
    async def test_memory_usage_under_load(self, ai_service, mock_image_data):
        """æ¸¬è©¦é«˜è² è¼‰ä¸‹çš„è¨˜æ†¶é«”ä½¿ç”¨"""
        
        try:
            import psutil
            process = psutil.Process()
        except ImportError:
            pytest.skip("psutil not available for memory testing")
        
        with patch('google.generativeai.GenerativeModel') as mock_model:
            mock_instance = mock_model.return_value
            mock_response = MagicMock()
            mock_response.text = '{"card_count": 1, "cards": [{"name": "Test"}], "overall_quality": "good"}'
            mock_instance.generate_content.return_value = mock_response
            
            # è¨˜éŒ„åˆå§‹è¨˜æ†¶é«”
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # é«˜è² è¼‰æ¸¬è©¦
            high_load_requests = 50
            tasks = []
            
            for i in range(high_load_requests):
                task = ai_service.process_image(
                    image_bytes=mock_image_data[i % len(mock_image_data)],
                    user_id=f"memory_test_user_{i}"
                )
                tasks.append(task)
            
            # åŸ·è¡Œæ‰€æœ‰è«‹æ±‚
            await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¼·åˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            await asyncio.sleep(1)  # è®“èƒŒæ™¯ä»»å‹™æœ‰æ™‚é–“æ¸…ç†
            
            # è¨˜éŒ„å³°å€¼è¨˜æ†¶é«”
            peak_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = peak_memory - initial_memory
            
            print(f"\nğŸ§  è¨˜æ†¶é«”ä½¿ç”¨æ¸¬è©¦:")
            print(f"   - åˆå§‹è¨˜æ†¶é«”: {initial_memory:.1f} MB")
            print(f"   - å³°å€¼è¨˜æ†¶é«”: {peak_memory:.1f} MB")
            print(f"   - è¨˜æ†¶é«”å¢é•·: {memory_increase:.1f} MB")
            print(f"   - æ¯è«‹æ±‚è¨˜æ†¶é«”: {memory_increase / high_load_requests:.3f} MB")
            
            # è¨˜æ†¶é«”ä½¿ç”¨åŸºæº–
            assert memory_increase < 200, f"è¨˜æ†¶é«”å¢é•·éå¤š: {memory_increase:.1f} MB"
            assert memory_increase / high_load_requests < 2, "æ¯è«‹æ±‚è¨˜æ†¶é«”ä½¿ç”¨éé«˜"

    @pytest.mark.asyncio
    async def test_error_recovery_performance(self, ai_service, mock_image_data):
        """æ¸¬è©¦éŒ¯èª¤æ¢å¾©æ•ˆèƒ½"""
        
        with patch('google.generativeai.GenerativeModel') as mock_model:
            mock_instance = mock_model.return_value
            
            # æ¨¡æ“¬é–“æ­‡æ€§éŒ¯èª¤
            call_count = 0
            def side_effect(*args, **kwargs):
                nonlocal call_count
                call_count += 1
                if call_count % 3 == 0:  # æ¯3æ¬¡å‘¼å«å¤±æ•—ä¸€æ¬¡
                    raise Exception("Simulated API error")
                
                mock_response = MagicMock()
                mock_response.text = '{"card_count": 1, "cards": [{"name": "Test"}], "overall_quality": "good"}'
                return mock_response
            
            mock_instance.generate_content.side_effect = side_effect
            
            # éŒ¯èª¤æ¢å¾©æ¸¬è©¦
            error_test_requests = 15
            start_time = time.time()
            
            tasks = []
            for i in range(error_test_requests):
                task = ai_service.process_image(
                    image_bytes=mock_image_data[i % len(mock_image_data)],
                    user_id=f"error_test_user_{i}"
                )
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            end_time = time.time()
            
            # åˆ†æçµæœ
            successful_results = []
            error_results = []
            
            for result in results:
                if isinstance(result, tuple):
                    result_data, metadata = result
                    if 'error' in result_data:
                        error_results.append(result)
                    else:
                        successful_results.append(result)
                else:
                    error_results.append(result)
            
            total_time = end_time - start_time
            success_rate = len(successful_results) / error_test_requests
            
            print(f"\nğŸ”„ éŒ¯èª¤æ¢å¾©æ•ˆèƒ½æ¸¬è©¦:")
            print(f"   - ç¸½è«‹æ±‚æ•¸: {error_test_requests}")
            print(f"   - æˆåŠŸè«‹æ±‚: {len(successful_results)}")
            print(f"   - å¤±æ•—è«‹æ±‚: {len(error_results)}")
            print(f"   - æˆåŠŸç‡: {success_rate:.1%}")
            print(f"   - ç¸½è™•ç†æ™‚é–“: {total_time:.2f}s")
            print(f"   - API å‘¼å«æ¬¡æ•¸: {call_count}")
            
            # é©—è­‰éŒ¯èª¤æ¢å¾©
            assert success_rate > 0.6, f"éŒ¯èª¤æ¢å¾©æˆåŠŸç‡éä½: {success_rate:.1%}"
            assert total_time < 60, f"éŒ¯èª¤æ¢å¾©æ™‚é–“éé•·: {total_time:.2f}s"

    @pytest.mark.asyncio
    async def test_service_health_monitoring(self, ai_service):
        """æ¸¬è©¦æœå‹™å¥åº·ç›£æ§"""
        
        # ç²å–æœå‹™ç‹€æ…‹
        service_status = await ai_service.get_service_status()
        health_check = await ai_service.health_check()
        
        print(f"\nğŸ¥ æœå‹™å¥åº·ç›£æ§æ¸¬è©¦:")
        print(f"   - æœå‹™é‹è¡Œç‹€æ…‹: {service_status['service_info']['running']}")
        print(f"   - èƒŒæ™¯ä»»å‹™æ•¸: {service_status['service_info']['background_tasks']}")
        print(f"   - å¥åº·ç‹€æ…‹: {health_check['status']}")
        print(f"   - å¯ç”¨ API Keys: {service_status['quota_status']['summary']['available_keys']}")
        
        # é©—è­‰å¥åº·ç‹€æ…‹
        assert service_status['service_info']['running'], "æœå‹™æ‡‰è©²è™•æ–¼é‹è¡Œç‹€æ…‹"
        assert service_status['quota_status']['summary']['available_keys'] > 0, "æ‡‰è©²æœ‰å¯ç”¨çš„ API Keys"
        assert health_check['status'] in ['healthy', 'degraded'], f"å¥åº·ç‹€æ…‹ç•°å¸¸: {health_check['status']}"

    @pytest.mark.asyncio
    async def test_performance_optimization(self, ai_service):
        """æ¸¬è©¦æ•ˆèƒ½æœ€ä½³åŒ–åŠŸèƒ½"""
        
        # åŸ·è¡Œæ•ˆèƒ½æœ€ä½³åŒ–
        optimization_result = await ai_service.optimize_performance()
        
        print(f"\nâš¡ æ•ˆèƒ½æœ€ä½³åŒ–æ¸¬è©¦:")
        print(f"   - æœ€ä½³åŒ–æˆåŠŸ: {optimization_result['success']}")
        print(f"   - åŸ·è¡Œé …ç›®: {optimization_result.get('optimizations_performed', [])}")
        
        # ç²å–æœ€ä½³åŒ–å»ºè­°
        recommendations = ai_service.get_optimization_recommendations()
        print(f"   - å»ºè­°æ•¸é‡: {len(recommendations)}")
        
        # é©—è­‰æœ€ä½³åŒ–åŠŸèƒ½
        assert optimization_result['success'], "æ•ˆèƒ½æœ€ä½³åŒ–æ‡‰è©²æˆåŠŸ"
        assert 'optimizations_performed' in optimization_result, "æ‡‰è©²åŒ…å«æœ€ä½³åŒ–é …ç›®"

    @pytest.mark.asyncio
    async def test_load_testing_stability(self, ai_service, mock_image_data):
        """è² è¼‰æ¸¬è©¦ç©©å®šæ€§"""
        
        with patch('google.generativeai.GenerativeModel') as mock_model:
            mock_instance = mock_model.return_value
            mock_response = MagicMock()
            mock_response.text = '{"card_count": 1, "cards": [{"name": "Test"}], "overall_quality": "good"}'
            mock_instance.generate_content.return_value = mock_response
            
            # æ¨¡æ“¬æŒçºŒè² è¼‰
            duration_seconds = 30
            requests_per_second = 5
            total_requests = duration_seconds * requests_per_second
            
            print(f"\nğŸ”¥ è² è¼‰æ¸¬è©¦ ({total_requests} è«‹æ±‚, {duration_seconds}s):")
            
            start_time = time.time()
            completed_requests = 0
            errors = 0
            
            # åˆ†æ‰¹åŸ·è¡Œä»¥æ§åˆ¶è² è¼‰
            batch_size = 10
            for batch_start in range(0, total_requests, batch_size):
                batch_end = min(batch_start + batch_size, total_requests)
                batch_tasks = []
                
                for i in range(batch_start, batch_end):
                    task = ai_service.process_image(
                        image_bytes=mock_image_data[i % len(mock_image_data)],
                        user_id=f"load_test_user_{i}"
                    )
                    batch_tasks.append(task)
                
                # åŸ·è¡Œæ‰¹æ¬¡
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                
                # çµ±è¨ˆçµæœ
                for result in batch_results:
                    completed_requests += 1
                    if isinstance(result, Exception):
                        errors += 1
                
                # æ§åˆ¶è«‹æ±‚é€Ÿç‡
                elapsed = time.time() - start_time
                expected_time = batch_end / requests_per_second
                if elapsed < expected_time:
                    await asyncio.sleep(expected_time - elapsed)
            
            total_time = time.time() - start_time
            actual_rps = completed_requests / total_time
            error_rate = errors / completed_requests
            
            print(f"   - å®Œæˆè«‹æ±‚: {completed_requests}")
            print(f"   - éŒ¯èª¤æ•¸é‡: {errors}")
            print(f"   - å¯¦éš› RPS: {actual_rps:.2f}")
            print(f"   - éŒ¯èª¤ç‡: {error_rate:.1%}")
            print(f"   - ç¸½æ™‚é–“: {total_time:.1f}s")
            
            # ç©©å®šæ€§é©—è­‰
            assert error_rate < 0.05, f"è² è¼‰æ¸¬è©¦éŒ¯èª¤ç‡éé«˜: {error_rate:.1%}"
            assert completed_requests >= total_requests * 0.95, "å®Œæˆç‡ä½æ–¼95%"
            assert actual_rps >= requests_per_second * 0.8, f"å¯¦éš› RPS éä½: {actual_rps:.2f}"


@pytest.mark.benchmark
class TestPerformanceBenchmarks:
    """æ•ˆèƒ½åŸºæº–æ¸¬è©¦"""
    
    @pytest.mark.asyncio
    async def test_response_time_benchmarks(self):
        """å›æ‡‰æ™‚é–“åŸºæº–æ¸¬è©¦"""
        
        # é€™è£¡å¯ä»¥æ·»åŠ èˆ‡åŸå§‹å¯¦ä½œçš„æ¯”è¼ƒæ¸¬è©¦
        # ç›®æ¨™ï¼šè­‰æ˜æœ€ä½³åŒ–ç‰ˆæœ¬çš„æ•ˆèƒ½æå‡
        
        print("ğŸ“Š æ•ˆèƒ½åŸºæº–æ¸¬è©¦çµæœ:")
        print("   - é æœŸå¹³å‡å›æ‡‰æ™‚é–“: < 5s")
        print("   - é æœŸ P95 å›æ‡‰æ™‚é–“: < 15s") 
        print("   - é æœŸä½µç™¼ååé‡: > 5 req/s")
        print("   - é æœŸå¿«å–å‘½ä¸­ç‡: > 30%")
        print("   - é æœŸéŒ¯èª¤ç‡: < 5%")


if __name__ == "__main__":
    # åŸ·è¡Œæ•ˆèƒ½æ¸¬è©¦
    pytest.main([__file__, "-v", "-s", "--tb=short"])