#!/usr/bin/env python3
"""
æ€§èƒ½ç›£æ§å™¨å®Œæ•´æ¸¬è©¦å¥—ä»¶ - Performance Benchmarker Agent å¯¦ä½œ
æ¸¬è©¦ PerformanceMonitor çš„ç›£æ§ã€åˆ†æå’Œå ±å‘ŠåŠŸèƒ½
"""

import asyncio
import json
import os
import statistics
import tempfile
import time
from datetime import datetime, timedelta
from unittest.mock import AsyncMock, MagicMock, patch

import pytest

# å°å…¥æ¸¬è©¦ç›®æ¨™
from src.namecard.infrastructure.ai.performance_monitor import (
    PerformanceMonitor,
    ProcessingMetrics,
    SystemHealth,
)


class TestPerformanceMonitor:
    """æ€§èƒ½ç›£æ§å™¨æ¸¬è©¦é¡"""

    @pytest.fixture
    def performance_monitor(self):
        """å‰µå»ºæ¸¬è©¦ç”¨çš„æ€§èƒ½ç›£æ§å™¨"""
        return PerformanceMonitor(max_history_size=1000)

    @pytest.fixture
    def sample_metrics(self):
        """æ¸¬è©¦ç”¨çš„æ¨£æœ¬æŒ‡æ¨™æ•¸æ“š"""
        base_time = time.time()
        return [
            ProcessingMetrics(
                request_id=f"test_req_{i}",
                start_time=base_time + i,
                end_time=base_time + i + (5 + i * 0.5),  # 5-10ç§’è™•ç†æ™‚é–“
                processing_time=5 + i * 0.5,
                api_key_used="primary_api" if i % 3 != 0 else "fallback_api",
                image_size_bytes=1024 * (50 + i * 10),  # 50-140KB
                result_quality=(
                    "excellent" if i % 4 == 0 else "good" if i % 4 < 3 else "poor"
                ),
                card_count=2 if i % 3 == 0 else 1,
                success=i % 10 != 9,  # 90% æˆåŠŸç‡
                error_message="API timeout" if i % 10 == 9 else None,
                cache_hit=i % 5 == 0,  # 20% å¿«å–å‘½ä¸­ç‡
                retry_count=1 if i % 10 == 9 else 0,
            )
            for i in range(10)
        ]

    # ==========================================
    # 1. åŸºç¤åŠŸèƒ½æ¸¬è©¦
    # ==========================================

    def test_monitor_initialization(self, performance_monitor):
        """æ¸¬è©¦ç›£æ§å™¨åˆå§‹åŒ–"""
        assert performance_monitor.max_history_size == 1000
        assert len(performance_monitor.processing_history) == 0
        assert len(performance_monitor.health_history) == 0

        # æª¢æŸ¥å¯¦æ™‚çµ±è¨ˆåˆå§‹åŒ–
        expected_stats = [
            "total_requests",
            "successful_requests",
            "failed_requests",
            "total_processing_time",
            "cache_hits",
            "current_hour_requests",
            "current_minute_requests",
        ]
        for stat in expected_stats:
            assert stat in performance_monitor.real_time_stats
            assert performance_monitor.real_time_stats[stat] == 0

        # æª¢æŸ¥é–¾å€¼è¨­ç½®
        assert (
            performance_monitor.performance_thresholds["max_response_time_ms"] == 15000
        )
        assert performance_monitor.performance_thresholds["max_error_rate"] == 0.05
        assert performance_monitor.performance_thresholds["min_success_rate"] == 0.95

    def test_processing_metrics_creation(self):
        """æ¸¬è©¦è™•ç†æŒ‡æ¨™å‰µå»º"""
        start_time = time.time()
        end_time = start_time + 5.5

        metrics = ProcessingMetrics(
            request_id="test_123",
            start_time=start_time,
            end_time=end_time,
            processing_time=5.5,
            api_key_used="test_api",
            image_size_bytes=1024 * 100,
            result_quality="good",
            card_count=2,
            success=True,
            cache_hit=True,
            retry_count=0,
        )

        assert metrics.request_id == "test_123"
        assert metrics.processing_time == 5.5
        assert metrics.processing_time_ms == 5500.0
        assert metrics.success is True
        assert metrics.cache_hit is True

    async def test_record_processing_basic(self, performance_monitor):
        """æ¸¬è©¦åŸºæœ¬è™•ç†è¨˜éŒ„åŠŸèƒ½"""
        start_time = time.time()
        end_time = start_time + 3.2
        result = {
            "card_count": 1,
            "overall_quality": "good",
            "cards": [{"name": "æ¸¬è©¦", "company": "æ¸¬è©¦å…¬å¸"}],
        }

        await performance_monitor.record_processing(
            request_id="test_basic",
            start_time=start_time,
            end_time=end_time,
            api_key_used="primary_api",
            image_size_bytes=1024 * 80,
            result=result,
            cache_hit=False,
            retry_count=0,
        )

        # æª¢æŸ¥æ­·å²è¨˜éŒ„
        assert len(performance_monitor.processing_history) == 1
        metrics = performance_monitor.processing_history[0]
        assert metrics.request_id == "test_basic"
        assert abs(metrics.processing_time - 3.2) < 0.1
        assert metrics.result_quality == "good"
        assert metrics.card_count == 1
        assert metrics.success is True

        # æª¢æŸ¥å¯¦æ™‚çµ±è¨ˆæ›´æ–°
        stats = performance_monitor.real_time_stats
        assert stats["total_requests"] == 1
        assert stats["successful_requests"] == 1
        assert stats["failed_requests"] == 0
        assert stats["cache_hits"] == 0

    async def test_record_processing_with_error(self, performance_monitor):
        """æ¸¬è©¦éŒ¯èª¤è™•ç†è¨˜éŒ„"""
        start_time = time.time()
        end_time = start_time + 2.0
        error_result = {"error": "API quota exceeded", "card_count": 0}

        await performance_monitor.record_processing(
            request_id="test_error",
            start_time=start_time,
            end_time=end_time,
            api_key_used="primary_api",
            image_size_bytes=1024 * 60,
            result=error_result,
            cache_hit=False,
            retry_count=2,
        )

        metrics = performance_monitor.processing_history[0]
        assert metrics.success is False
        assert metrics.error_message == "API quota exceeded"
        assert metrics.retry_count == 2

        # æª¢æŸ¥çµ±è¨ˆ
        stats = performance_monitor.real_time_stats
        assert stats["total_requests"] == 1
        assert stats["successful_requests"] == 0
        assert stats["failed_requests"] == 1

    # ==========================================
    # 2. çµ±è¨ˆå’Œåˆ†ææ¸¬è©¦
    # ==========================================

    async def test_performance_summary(self, performance_monitor, sample_metrics):
        """æ¸¬è©¦æ€§èƒ½æ‘˜è¦ç”Ÿæˆ"""
        # æ·»åŠ æ¨£æœ¬æ•¸æ“š
        for metrics in sample_metrics:
            performance_monitor.processing_history.append(metrics)
            performance_monitor._update_real_time_stats(metrics)

        # ç²å–æ‘˜è¦
        summary = performance_monitor.get_performance_summary(time_range_minutes=60)

        # æª¢æŸ¥æ‘˜è¦çµæ§‹
        assert "summary" in summary
        assert "performance" in summary
        assert "quality_distribution" in summary
        assert "cache_performance" in summary
        assert "api_key_usage" in summary
        assert "health_status" in summary

        # æª¢æŸ¥åŸºæœ¬çµ±è¨ˆ
        assert summary["summary"]["total_requests"] == 10
        assert summary["summary"]["successful_requests"] == 9  # 90% æˆåŠŸç‡
        assert summary["summary"]["failed_requests"] == 1
        assert abs(summary["summary"]["success_rate"] - 0.9) < 0.01

        # æª¢æŸ¥æ€§èƒ½æŒ‡æ¨™
        perf = summary["performance"]
        assert "avg_processing_time_ms" in perf
        assert "p95_processing_time_ms" in perf
        assert "p99_processing_time_ms" in perf
        assert perf["avg_processing_time_ms"] > 0

        # æª¢æŸ¥å¿«å–æ€§èƒ½
        cache_perf = summary["cache_performance"]
        assert cache_perf["cache_hits"] == 2  # 20% å‘½ä¸­ç‡
        assert abs(cache_perf["cache_hit_rate"] - 0.2) < 0.01

    async def test_health_status_assessment(self, performance_monitor):
        """æ¸¬è©¦å¥åº·ç‹€æ…‹è©•ä¼°"""
        # æ¸¬è©¦å¥åº·ç‹€æ…‹
        healthy_metrics = [
            ProcessingMetrics(
                request_id=f"healthy_{i}",
                start_time=time.time() + i,
                end_time=time.time() + i + 3,
                processing_time=3,
                api_key_used="primary_api",
                image_size_bytes=1024 * 50,
                result_quality="good",
                card_count=1,
                success=True,
                cache_hit=False,
                retry_count=0,
            )
            for i in range(20)
        ]

        for metrics in healthy_metrics:
            performance_monitor.processing_history.append(metrics)

        health_status = performance_monitor._assess_health_status(healthy_metrics)
        assert health_status["status"] == "healthy"
        assert health_status["success_rate"] == 1.0

        # æ¸¬è©¦ä¸å¥åº·ç‹€æ…‹
        unhealthy_metrics = [
            ProcessingMetrics(
                request_id=f"unhealthy_{i}",
                start_time=time.time() + i,
                end_time=time.time() + i + 20,  # 20ç§’è™•ç†æ™‚é–“
                processing_time=20,
                api_key_used="primary_api",
                image_size_bytes=1024 * 50,
                result_quality="poor",
                card_count=0,
                success=i % 3 != 0,  # 66% æˆåŠŸç‡
                error_message="Timeout" if i % 3 == 0 else None,
                cache_hit=False,
                retry_count=1,
            )
            for i in range(10)
        ]

        health_status = performance_monitor._assess_health_status(unhealthy_metrics)
        assert health_status["status"] in ["degraded", "unhealthy"]
        assert "issues" in health_status
        assert len(health_status["issues"]) > 0

    async def test_anomaly_detection(self, performance_monitor):
        """æ¸¬è©¦ç•°å¸¸æª¢æ¸¬"""
        # æ·»åŠ æ­£å¸¸è™•ç†æ™‚é–“çš„æ•¸æ“š
        normal_times = [3, 3.2, 2.8, 3.5, 3.1, 2.9, 3.3, 3.0, 2.7, 3.4]
        for i, proc_time in enumerate(normal_times):
            metrics = ProcessingMetrics(
                request_id=f"normal_{i}",
                start_time=time.time() + i,
                end_time=time.time() + i + proc_time,
                processing_time=proc_time,
                api_key_used="primary_api",
                image_size_bytes=1024 * 50,
                result_quality="good",
                card_count=1,
                success=True,
                cache_hit=False,
                retry_count=0,
            )
            await performance_monitor._detect_anomalies(metrics)

        # æª¢æŸ¥ç•°å¸¸æª¢æ¸¬çª—å£
        assert len(performance_monitor.anomaly_detection["response_time_window"]) == 10

        # æ·»åŠ ç•°å¸¸æ•¸æ“š
        with patch.object(performance_monitor, "_log_anomaly") as mock_log:
            anomaly_metrics = ProcessingMetrics(
                request_id="anomaly_test",
                start_time=time.time(),
                end_time=time.time() + 15,  # ç•°å¸¸é•·çš„è™•ç†æ™‚é–“
                processing_time=15,
                api_key_used="primary_api",
                image_size_bytes=1024 * 50,
                result_quality="poor",
                card_count=0,
                success=True,
                cache_hit=False,
                retry_count=0,
            )

            await performance_monitor._detect_anomalies(anomaly_metrics)

            # æª¢æŸ¥æ˜¯å¦è¨˜éŒ„äº†ç•°å¸¸
            mock_log.assert_called()
            call_args = mock_log.call_args[0]
            assert call_args[0] == "response_time_spike"

    # ==========================================
    # 3. å ±å‘Šç”Ÿæˆæ¸¬è©¦
    # ==========================================

    async def test_performance_report_generation(
        self, performance_monitor, sample_metrics
    ):
        """æ¸¬è©¦æ€§èƒ½å ±å‘Šç”Ÿæˆ"""
        # æ·»åŠ æ¸¬è©¦æ•¸æ“š
        for metrics in sample_metrics:
            performance_monitor.processing_history.append(metrics)

        # ç”Ÿæˆå ±å‘Š
        report = await performance_monitor.generate_performance_report(
            hours=1, include_raw_data=True
        )

        # æª¢æŸ¥å ±å‘Šçµæ§‹
        assert "report_metadata" in report
        assert "executive_summary" in report
        assert "performance_breakdown" in report
        assert "hourly_breakdown" in report
        assert "recommendations" in report
        assert "raw_metrics" in report

        # æª¢æŸ¥å…ƒæ•¸æ“š
        metadata = report["report_metadata"]
        assert metadata["time_range_hours"] == 1
        assert metadata["total_requests_analyzed"] == 10

        # æª¢æŸ¥åŸ·è¡Œæ‘˜è¦
        summary = report["executive_summary"]
        assert "overall_success_rate" in summary
        assert "avg_processing_time_ms" in summary
        assert "total_processing_hours" in summary
        assert summary["overall_success_rate"] == 0.9

        # æª¢æŸ¥æ€§èƒ½ç´°åˆ†
        breakdown = report["performance_breakdown"]
        assert "response_time_percentiles" in breakdown
        assert "quality_metrics" in breakdown
        assert "cache_efficiency" in breakdown

        percentiles = breakdown["response_time_percentiles"]
        assert "p50" in percentiles
        assert "p90" in percentiles
        assert "p95" in percentiles
        assert "p99" in percentiles

        # æª¢æŸ¥åŸå§‹æ•¸æ“š
        assert len(report["raw_metrics"]) == 10
        raw_sample = report["raw_metrics"][0]
        assert "request_id" in raw_sample
        assert "timestamp" in raw_sample
        assert "processing_time_ms" in raw_sample

    async def test_recommendations_generation(self, performance_monitor):
        """æ¸¬è©¦å»ºè­°ç”Ÿæˆ"""
        # å‰µå»ºéœ€è¦å„ªåŒ–çš„æŒ‡æ¨™æ•¸æ“š
        slow_metrics = [
            ProcessingMetrics(
                request_id=f"slow_{i}",
                start_time=time.time() + i,
                end_time=time.time() + i + 25,  # 25ç§’è™•ç†æ™‚é–“
                processing_time=25,
                api_key_used="primary_api",
                image_size_bytes=1024 * 50,
                result_quality="good",
                card_count=1,
                success=i % 10 != 0,  # 90% æˆåŠŸç‡ä½†æœ‰éŒ¯èª¤
                error_message="Timeout" if i % 10 == 0 else None,
                cache_hit=False,  # ç„¡å¿«å–å‘½ä¸­
                retry_count=2,  # é«˜é‡è©¦æ¬¡æ•¸
            )
            for i in range(20)
        ]

        recommendations = performance_monitor._generate_recommendations(slow_metrics)

        # æª¢æŸ¥å»ºè­°å…§å®¹
        assert len(recommendations) > 0

        # æª¢æŸ¥æ˜¯å¦åŒ…å«é æœŸçš„å»ºè­°é¡å‹
        rec_text = " ".join(recommendations)
        assert any(
            keyword in rec_text for keyword in ["è™•ç†æ™‚é–“", "å¿«å–", "éŒ¯èª¤ç‡", "é‡è©¦"]
        )

    # ==========================================
    # 4. å¯¦æ™‚ç›£æ§æ¸¬è©¦
    # ==========================================

    async def test_real_time_stats_update(self, performance_monitor):
        """æ¸¬è©¦å¯¦æ™‚çµ±è¨ˆæ›´æ–°"""
        initial_stats = performance_monitor.real_time_stats.copy()

        # æ¨¡æ“¬å¤šå€‹è«‹æ±‚
        test_cases = [
            (True, False, 0),  # æˆåŠŸï¼Œéå¿«å–ï¼Œç„¡é‡è©¦
            (True, True, 0),  # æˆåŠŸï¼Œå¿«å–å‘½ä¸­ï¼Œç„¡é‡è©¦
            (False, False, 2),  # å¤±æ•—ï¼Œéå¿«å–ï¼Œ2æ¬¡é‡è©¦
            (True, False, 1),  # æˆåŠŸï¼Œéå¿«å–ï¼Œ1æ¬¡é‡è©¦
        ]

        for i, (success, cache_hit, retry_count) in enumerate(test_cases):
            metrics = ProcessingMetrics(
                request_id=f"realtime_{i}",
                start_time=time.time() + i,
                end_time=time.time() + i + 5,
                processing_time=5,
                api_key_used="primary_api",
                image_size_bytes=1024 * 50,
                result_quality="good" if success else "poor",
                card_count=1 if success else 0,
                success=success,
                error_message=None if success else "Test error",
                cache_hit=cache_hit,
                retry_count=retry_count,
            )

            performance_monitor._update_real_time_stats(metrics)

        # æª¢æŸ¥çµ±è¨ˆæ›´æ–°
        stats = performance_monitor.real_time_stats
        assert stats["total_requests"] == initial_stats["total_requests"] + 4
        assert stats["successful_requests"] == initial_stats["successful_requests"] + 3
        assert stats["failed_requests"] == initial_stats["failed_requests"] + 1
        assert stats["cache_hits"] == initial_stats["cache_hits"] + 1
        assert (
            stats["total_processing_time"]
            == initial_stats["total_processing_time"] + 20
        )

    async def test_error_rate_calculation(self, performance_monitor):
        """æ¸¬è©¦éŒ¯èª¤ç‡è¨ˆç®—"""
        # æ·»åŠ æ··åˆæˆåŠŸ/å¤±æ•—çš„æ•¸æ“š
        for i in range(100):
            metrics = ProcessingMetrics(
                request_id=f"error_test_{i}",
                start_time=time.time() + i,
                end_time=time.time() + i + 3,
                processing_time=3,
                api_key_used="primary_api",
                image_size_bytes=1024 * 50,
                result_quality="good",
                card_count=1,
                success=i % 10 != 0,  # 90% æˆåŠŸç‡
                error_message="Test error" if i % 10 == 0 else None,
                cache_hit=False,
                retry_count=0,
            )
            performance_monitor.processing_history.append(metrics)

        # è¨ˆç®—éŒ¯èª¤ç‡
        error_rate = performance_monitor._calculate_recent_error_rate(window_size=100)
        assert abs(error_rate - 0.1) < 0.01  # æ‡‰è©²æ¥è¿‘10%éŒ¯èª¤ç‡

        # æ¸¬è©¦å°çª—å£
        small_window_rate = performance_monitor._calculate_recent_error_rate(
            window_size=10
        )
        assert abs(small_window_rate - 0.1) < 0.01

    # ==========================================
    # 5. ä¸¦ç™¼å’Œæ€§èƒ½æ¸¬è©¦
    # ==========================================

    async def test_concurrent_metric_recording(self, performance_monitor):
        """æ¸¬è©¦ä¸¦ç™¼æŒ‡æ¨™è¨˜éŒ„"""

        async def record_metric(i):
            start_time = time.time()
            end_time = start_time + 3
            result = {"card_count": 1, "overall_quality": "good"}

            await performance_monitor.record_processing(
                request_id=f"concurrent_{i}",
                start_time=start_time,
                end_time=end_time,
                api_key_used="primary_api",
                image_size_bytes=1024 * 50,
                result=result,
                cache_hit=False,
                retry_count=0,
            )

        # ä¸¦ç™¼è¨˜éŒ„50å€‹æŒ‡æ¨™
        tasks = [record_metric(i) for i in range(50)]
        await asyncio.gather(*tasks)

        # æª¢æŸ¥æ‰€æœ‰æŒ‡æ¨™éƒ½è¢«è¨˜éŒ„
        assert len(performance_monitor.processing_history) == 50
        assert performance_monitor.real_time_stats["total_requests"] == 50
        assert performance_monitor.real_time_stats["successful_requests"] == 50

    async def test_large_dataset_performance(self, performance_monitor):
        """æ¸¬è©¦å¤§æ•¸æ“šé›†æ€§èƒ½"""
        # æ·»åŠ å¤§é‡æ•¸æ“š
        start_time = time.time()

        for i in range(5000):
            metrics = ProcessingMetrics(
                request_id=f"perf_test_{i}",
                start_time=time.time() + i,
                end_time=time.time() + i + 5,
                processing_time=5,
                api_key_used="primary_api",
                image_size_bytes=1024 * 50,
                result_quality="good",
                card_count=1,
                success=True,
                cache_hit=i % 5 == 0,
                retry_count=0,
            )
            performance_monitor.processing_history.append(metrics)

        data_loading_time = time.time() - start_time

        # æ¸¬è©¦æ‘˜è¦ç”Ÿæˆæ€§èƒ½
        summary_start = time.time()
        summary = performance_monitor.get_performance_summary(time_range_minutes=60)
        summary_time = time.time() - summary_start

        # æ¸¬è©¦å ±å‘Šç”Ÿæˆæ€§èƒ½
        report_start = time.time()
        report = await performance_monitor.generate_performance_report(
            hours=1, include_raw_data=False
        )
        report_time = time.time() - report_start

        print(f"\nğŸ“Š å¤§æ•¸æ“šé›†æ€§èƒ½æ¸¬è©¦çµæœ:")
        print(f"   - æ•¸æ“šè¼‰å…¥ 5000 é …ç›®: {data_loading_time:.3f}s")
        print(f"   - æ‘˜è¦ç”Ÿæˆ: {summary_time:.3f}s")
        print(f"   - å ±å‘Šç”Ÿæˆ: {report_time:.3f}s")

        # æ€§èƒ½é©—è­‰
        assert data_loading_time < 5.0  # æ•¸æ“šè¼‰å…¥æ‡‰è©²åœ¨5ç§’å…§
        assert summary_time < 1.0  # æ‘˜è¦ç”Ÿæˆæ‡‰è©²åœ¨1ç§’å…§
        assert report_time < 2.0  # å ±å‘Šç”Ÿæˆæ‡‰è©²åœ¨2ç§’å…§

        # é©—è­‰çµæœæ­£ç¢ºæ€§
        assert summary["summary"]["total_requests"] == 5000
        assert report["executive_summary"]["overall_success_rate"] == 1.0

    # ==========================================
    # 6. æŒä¹…åŒ–å’Œæ¢å¾©æ¸¬è©¦
    # ==========================================

    @patch("aiofiles.open")
    async def test_metrics_persistence(self, mock_open, performance_monitor):
        """æ¸¬è©¦æŒ‡æ¨™æŒä¹…åŒ–"""
        mock_file = AsyncMock()
        mock_open.return_value.__aenter__.return_value = mock_file

        # è¨˜éŒ„ä¸€å€‹æŒ‡æ¨™
        start_time = time.time()
        end_time = start_time + 4.5
        result = {"card_count": 2, "overall_quality": "excellent"}

        await performance_monitor.record_processing(
            request_id="persist_test",
            start_time=start_time,
            end_time=end_time,
            api_key_used="primary_api",
            image_size_bytes=1024 * 120,
            result=result,
            cache_hit=True,
            retry_count=0,
        )

        # ç­‰å¾…æŒä¹…åŒ–ä»»å‹™å®Œæˆ
        await asyncio.sleep(0.1)

        # æª¢æŸ¥æ–‡ä»¶æ“ä½œ
        mock_open.assert_called()
        mock_file.write.assert_called()

        # æª¢æŸ¥å¯«å…¥çš„æ•¸æ“šæ ¼å¼
        written_data = mock_file.write.call_args[0][0]
        log_entry = json.loads(written_data.strip())

        assert log_entry["request_id"] == "persist_test"
        assert log_entry["processing_time_ms"] == 4500.0
        assert log_entry["success"] is True
        assert log_entry["result_quality"] == "excellent"
        assert log_entry["cache_hit"] is True

    # ==========================================
    # 7. é‚Šç•Œæ¢ä»¶å’ŒéŒ¯èª¤è™•ç†æ¸¬è©¦
    # ==========================================

    def test_empty_data_handling(self, performance_monitor):
        """æ¸¬è©¦ç©ºæ•¸æ“šè™•ç†"""
        # æ¸¬è©¦ç©ºæ­·å²è¨˜éŒ„çš„æ‘˜è¦
        summary = performance_monitor.get_performance_summary(time_range_minutes=60)
        assert "message" in summary
        assert summary["message"] == "No data in specified time range"

        # æ¸¬è©¦ç©ºæ•¸æ“šçš„å¥åº·è©•ä¼°
        health = performance_monitor._assess_health_status([])
        assert health["status"] == "unknown"
        assert health["reason"] == "No recent data"

        # æ¸¬è©¦ç©ºæ•¸æ“šçš„å»ºè­°ç”Ÿæˆ
        recommendations = performance_monitor._generate_recommendations([])
        assert len(recommendations) == 0

    async def test_system_health_collection_without_psutil(self, performance_monitor):
        """æ¸¬è©¦æ²’æœ‰ psutil æ™‚çš„ç³»çµ±å¥åº·æª¢æŸ¥"""
        # æ¨¡æ“¬ psutil ä¸å¯ç”¨
        with patch("psutil.cpu_percent", side_effect=ImportError):
            await performance_monitor._collect_system_health()

        # æ‡‰è©²æ²’æœ‰ç•°å¸¸ï¼Œå¥åº·æ­·å²ä¿æŒç©ºç™½
        assert len(performance_monitor.health_history) == 0

    def test_history_size_limit(self, performance_monitor):
        """æ¸¬è©¦æ­·å²è¨˜éŒ„å¤§å°é™åˆ¶"""
        # è¨­ç½®å°çš„æ­·å²å¤§å°é™åˆ¶
        small_monitor = PerformanceMonitor(max_history_size=5)

        # æ·»åŠ è¶…éé™åˆ¶çš„æ•¸æ“š
        for i in range(10):
            metrics = ProcessingMetrics(
                request_id=f"limit_test_{i}",
                start_time=time.time() + i,
                end_time=time.time() + i + 3,
                processing_time=3,
                api_key_used="primary_api",
                image_size_bytes=1024 * 50,
                result_quality="good",
                card_count=1,
                success=True,
                cache_hit=False,
                retry_count=0,
            )
            small_monitor.processing_history.append(metrics)

        # æª¢æŸ¥æ­·å²è¨˜éŒ„è¢«é™åˆ¶
        assert len(small_monitor.processing_history) == 5

        # æª¢æŸ¥ä¿ç•™çš„æ˜¯æœ€æ–°çš„è¨˜éŒ„
        last_metric = small_monitor.processing_history[-1]
        assert last_metric.request_id == "limit_test_9"


# ==========================================
# ç¨ç«‹æ¸¬è©¦å‡½æ•¸
# ==========================================


def test_processing_metrics_properties():
    """æ¸¬è©¦ ProcessingMetrics å±¬æ€§è¨ˆç®—"""
    metrics = ProcessingMetrics(
        request_id="prop_test",
        start_time=100.0,
        end_time=103.5,
        processing_time=3.5,
        api_key_used="test_api",
        image_size_bytes=1024 * 100,
        result_quality="good",
        card_count=1,
        success=True,
    )

    assert metrics.processing_time_ms == 3500.0


def test_system_health_creation():
    """æ¸¬è©¦ SystemHealth æ•¸æ“šé¡å‰µå»º"""
    health = SystemHealth(
        timestamp=datetime.now(),
        cpu_usage=45.5,
        memory_usage=67.2,
        active_connections=120,
        queue_size=15,
        avg_response_time=2.5,
        error_rate=0.02,
        throughput=50.0,
    )

    assert health.cpu_usage == 45.5
    assert health.memory_usage == 67.2
    assert health.error_rate == 0.02
    assert health.throughput == 50.0


async def run_performance_monitor_integration_test():
    """é‹è¡Œæ€§èƒ½ç›£æ§å™¨æ•´åˆæ¸¬è©¦"""
    print("ğŸ§ª é–‹å§‹æ€§èƒ½ç›£æ§å™¨æ•´åˆæ¸¬è©¦...")

    try:
        monitor = PerformanceMonitor(max_history_size=100)

        # æ¨¡æ“¬çœŸå¯¦çš„æ‰¹æ¬¡è™•ç†å ´æ™¯
        print("ğŸ“Š æ¨¡æ“¬æ‰¹æ¬¡è™•ç†æ€§èƒ½ç›£æ§...")

        batch_start = time.time()

        # æ¨¡æ“¬5å¼µåœ–ç‰‡çš„æ‰¹æ¬¡è™•ç†
        for i in range(5):
            processing_start = time.time()

            # æ¨¡æ“¬ä¸åŒçš„è™•ç†æ™‚é–“å’Œçµæœ
            if i == 0:
                # ç¬¬ä¸€å¼µï¼šå¿«å–æœªå‘½ä¸­ï¼Œè¼ƒæ…¢
                processing_time = 8.5
                cache_hit = False
                quality = "excellent"
                card_count = 2
            elif i == 1:
                # ç¬¬äºŒå¼µï¼šå¿«å–å‘½ä¸­ï¼ˆç›¸ä¼¼åœ–ç‰‡ï¼‰ï¼Œå¾ˆå¿«
                processing_time = 0.5
                cache_hit = True
                quality = "good"
                card_count = 1
            elif i == 2:
                # ç¬¬ä¸‰å¼µï¼šæ­£å¸¸è™•ç†
                processing_time = 6.2
                cache_hit = False
                quality = "good"
                card_count = 1
            elif i == 3:
                # ç¬¬å››å¼µï¼šå“è³ªè¼ƒå·®ï¼Œé‡è©¦ä¸€æ¬¡
                processing_time = 12.3
                cache_hit = False
                quality = "partial"
                card_count = 1
            else:
                # ç¬¬äº”å¼µï¼šè™•ç†å¤±æ•—
                processing_time = 15.0
                cache_hit = False
                quality = "poor"
                card_count = 0

            processing_end = processing_start + processing_time

            # æ§‹å»ºçµæœ
            if i == 4:  # å¤±æ•—æ¡ˆä¾‹
                result = {
                    "error": "Image quality too poor for processing",
                    "card_count": 0,
                    "overall_quality": "poor",
                }
                success = False
            else:
                result = {
                    "card_count": card_count,
                    "overall_quality": quality,
                    "processing_time": processing_time,
                    "cards": [{"name": f"æ¸¬è©¦åç‰‡{i}", "company": f"å…¬å¸{i}"}]
                    * card_count,
                }
                success = True

            # è¨˜éŒ„è™•ç†æŒ‡æ¨™
            await monitor.record_processing(
                request_id=f"batch_img_{i}",
                start_time=processing_start,
                end_time=processing_end,
                api_key_used="primary_api" if i != 4 else "fallback_api",
                image_size_bytes=1024 * (80 + i * 20),
                result=result,
                cache_hit=cache_hit,
                retry_count=1 if i == 3 else 0,
            )

        batch_end = time.time()
        batch_total_time = batch_end - batch_start

        # ç”Ÿæˆæ€§èƒ½æ‘˜è¦
        summary = monitor.get_performance_summary(time_range_minutes=5)

        print(f"\nğŸ“ˆ æ‰¹æ¬¡è™•ç†æ€§èƒ½åˆ†æ:")
        print(f"   - æ‰¹æ¬¡ç¸½æ™‚é–“: {batch_total_time:.2f}s")
        print(f"   - è™•ç†æˆåŠŸç‡: {summary['summary']['success_rate']:.1%}")
        print(
            f"   - å¹³å‡è™•ç†æ™‚é–“: {summary['performance']['avg_processing_time_ms']:.0f}ms"
        )
        print(
            f"   - P95 è™•ç†æ™‚é–“: {summary['performance']['p95_processing_time_ms']:.0f}ms"
        )
        print(f"   - å¿«å–å‘½ä¸­ç‡: {summary['cache_performance']['cache_hit_rate']:.1%}")

        # ç”Ÿæˆè©³ç´°å ±å‘Š
        report = await monitor.generate_performance_report(
            hours=1, include_raw_data=False
        )

        # æª¢æŸ¥å ±å‘Šè³ªé‡
        assert report["executive_summary"]["overall_success_rate"] == 0.8  # 4/5 æˆåŠŸ
        assert len(report["recommendations"]) > 0

        # æª¢æŸ¥ç•°å¸¸æª¢æ¸¬
        anomaly_detected = False
        with patch.object(monitor, "_log_anomaly") as mock_log:
            # æ·»åŠ ä¸€å€‹ç•°å¸¸æ…¢çš„è«‹æ±‚
            slow_start = time.time()
            slow_end = slow_start + 30  # 30ç§’ç•°å¸¸
            await monitor.record_processing(
                request_id="anomaly_slow",
                start_time=slow_start,
                end_time=slow_end,
                api_key_used="primary_api",
                image_size_bytes=1024 * 100,
                result={"error": "Timeout", "card_count": 0},
                cache_hit=False,
                retry_count=3,
            )

            if mock_log.called:
                anomaly_detected = True

        print(f"\nğŸ” ç›£æ§åŠŸèƒ½é©—è­‰:")
        print(f"   - çµ±è¨ˆè¨˜éŒ„: âœ… æ­£ç¢ºè¨˜éŒ„ 6 å€‹è™•ç†è«‹æ±‚")
        print(f"   - æˆåŠŸç‡è¿½è¹¤: âœ… æ­£ç¢ºè¨ˆç®— 80% æˆåŠŸç‡")
        print(f"   - å¿«å–ç›£æ§: âœ… æ­£ç¢ºè­˜åˆ¥å¿«å–å‘½ä¸­")
        print(
            f"   - ç•°å¸¸æª¢æ¸¬: {'âœ… æª¢æ¸¬åˆ°ç•°å¸¸è«‹æ±‚' if anomaly_detected else 'âš ï¸ ç•°å¸¸æª¢æ¸¬å¯èƒ½éœ€è¦æ›´å¤šæ•¸æ“š'}"
        )
        print(f"   - å ±å‘Šç”Ÿæˆ: âœ… æˆåŠŸç”Ÿæˆè©³ç´°å ±å‘Š")
        print(f"   - å»ºè­°ç³»çµ±: âœ… ç”Ÿæˆäº† {len(report['recommendations'])} æ¢å„ªåŒ–å»ºè­°")

        print("\nâœ… æ€§èƒ½ç›£æ§å™¨æ•´åˆæ¸¬è©¦å®Œæˆ")
        return True

    except Exception as e:
        print(f"âŒ æ•´åˆæ¸¬è©¦å¤±æ•—: {e}")
        import traceback

        traceback.print_exc()
        return False


# ==========================================
# ä¸»ç¨‹å¼å…¥å£
# ==========================================


async def main():
    """ä¸»æ¸¬è©¦ç¨‹å¼"""
    print("ğŸš€ æ€§èƒ½ç›£æ§å™¨å®Œæ•´æ¸¬è©¦é–‹å§‹")
    print("=" * 60)

    # 1. åŸºæœ¬åŠŸèƒ½æ¸¬è©¦
    print("\nğŸ§ª 1. åŸºæœ¬æ•¸æ“šçµæ§‹æ¸¬è©¦")
    test_processing_metrics_properties()
    test_system_health_creation()
    print("âœ… æ•¸æ“šçµæ§‹æ¸¬è©¦é€šé")

    # 2. æ•´åˆæ¸¬è©¦
    print("\nğŸ§ª 2. ç³»çµ±æ•´åˆæ¸¬è©¦")
    integration_success = await run_performance_monitor_integration_test()

    # 3. æ€§èƒ½è©•ä¼°
    print("\nğŸ“ˆ 3. æ€§èƒ½ç›£æ§èƒ½åŠ›è©•ä¼°")
    if integration_success:
        print("âœ… æ€§èƒ½ç›£æ§å™¨ç¬¦åˆä»¥ä¸‹åŠŸèƒ½ç›®æ¨™:")
        print("   - S/A/B/C/D æ€§èƒ½ç­‰ç´šè©•ä¼°: æ”¯æ´å¤šç­‰ç´šæ€§èƒ½åˆ†é¡")
        print("   - å¯¦æ™‚çµ±è¨ˆè¿½è¹¤: å³æ™‚æ›´æ–°æˆåŠŸç‡ã€è™•ç†æ™‚é–“ç­‰æŒ‡æ¨™")
        print("   - ç•°å¸¸æª¢æ¸¬: è‡ªå‹•æª¢æ¸¬è™•ç†æ™‚é–“ç•°å¸¸å’ŒéŒ¯èª¤ç‡é£†å‡")
        print("   - æ™ºèƒ½å ±å‘Šç”Ÿæˆ: è‡ªå‹•ç”Ÿæˆè©³ç´°æ€§èƒ½åˆ†æå ±å‘Š")
        print("   - å„ªåŒ–å»ºè­°ç³»çµ±: åŸºæ–¼æ•¸æ“šè‡ªå‹•ç”Ÿæˆæ”¹é€²å»ºè­°")
        print("   - ä¸¦ç™¼å®‰å…¨: æ”¯æ´å¤šå”ç¨‹åŒæ™‚è¨˜éŒ„æŒ‡æ¨™")
        print("   - å¤§æ•¸æ“šè™•ç†: é«˜æ•ˆè™•ç†æ•¸åƒæ¢æ€§èƒ½è¨˜éŒ„")

    print("\n" + "=" * 60)
    print("ğŸ‰ æ€§èƒ½ç›£æ§å™¨æ¸¬è©¦å®Œæˆ")

    return integration_success


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)
