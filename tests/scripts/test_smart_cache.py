#!/usr/bin/env python3
"""
æ™ºèƒ½å¿«å–ç³»çµ±å®Œæ•´æ¸¬è©¦å¥—ä»¶ - Test Writer/Fixer Agent å¯¦ä½œ
æ¸¬è©¦ SmartCacheManager çš„ä¸‰å±¤å¿«å–æ¶æ§‹å’Œæ€§èƒ½è¡¨ç¾
"""

import asyncio
import json
import os
import pickle
import shutil
import tempfile
import time
from datetime import datetime, timedelta
from unittest.mock import AsyncMock, MagicMock, patch

import pytest

# å°å…¥æ¸¬è©¦ç›®æ¨™
from src.namecard.infrastructure.ai.smart_cache import CacheEntry, SmartCacheManager


class TestSmartCacheManager:
    """æ™ºèƒ½å¿«å–ç®¡ç†å™¨æ¸¬è©¦é¡"""

    @pytest.fixture
    async def cache_manager(self):
        """å‰µå»ºæ¸¬è©¦ç”¨çš„å¿«å–ç®¡ç†å™¨"""
        # ä½¿ç”¨è‡¨æ™‚ç›®éŒ„é¿å…æ±¡æŸ“
        temp_dir = tempfile.mkdtemp(prefix="test_cache_")

        manager = SmartCacheManager(
            max_memory_size_mb=1,  # 1MB è¨˜æ†¶é«”å¿«å–
            max_disk_size_mb=5,  # 5MB ç£ç¢Ÿå¿«å–
            redis_url=None,  # æ¸¬è©¦æ™‚ä¸ä½¿ç”¨ Redis
            cache_dir=temp_dir,
        )

        yield manager

        # æ¸…ç†
        shutil.rmtree(temp_dir, ignore_errors=True)

    @pytest.fixture
    def sample_data(self):
        """æ¸¬è©¦ç”¨çš„æ¨£æœ¬æ•¸æ“š"""
        return {
            "small_data": {"key": "value", "size": "small"},  # < 1KB
            "medium_data": {"data": "x" * 1024 * 10},  # ~10KB
            "large_data": {"data": "x" * 1024 * 100},  # ~100KB
            "huge_data": {"data": "x" * 1024 * 500},  # ~500KB
        }

    # ==========================================
    # 1. åŸºç¤åŠŸèƒ½æ¸¬è©¦
    # ==========================================

    async def test_cache_initialization(self, cache_manager):
        """æ¸¬è©¦å¿«å–ç®¡ç†å™¨åˆå§‹åŒ–"""
        assert cache_manager.max_memory_size == 1024 * 1024  # 1MB
        assert cache_manager.max_disk_size == 5 * 1024 * 1024  # 5MB
        assert len(cache_manager.memory_cache) == 0
        assert len(cache_manager.disk_cache_index) == 0
        assert cache_manager.redis_client is None

        # æª¢æŸ¥çµ±è¨ˆåˆå§‹åŒ–
        expected_stats = [
            "memory_hits",
            "redis_hits",
            "disk_hits",
            "misses",
            "evictions",
            "total_requests",
        ]
        for stat in expected_stats:
            assert stat in cache_manager.stats
            assert cache_manager.stats[stat] == 0

    async def test_memory_cache_basic_operations(self, cache_manager, sample_data):
        """æ¸¬è©¦è¨˜æ†¶é«”å¿«å–åŸºæœ¬æ“ä½œ"""
        key = "test_key"
        value = sample_data["small_data"]

        # æ¸¬è©¦è¨­å®š
        await cache_manager.set(key, value, ttl_seconds=60, cache_level="memory")

        # æª¢æŸ¥è¨˜æ†¶é«”å¿«å–
        assert key in cache_manager.memory_cache
        entry = cache_manager.memory_cache[key]
        assert entry.value == value
        assert entry.access_count == 1
        assert not entry.is_expired()

        # æ¸¬è©¦å–å€¼
        retrieved_value = await cache_manager.get(key)
        assert retrieved_value == value

        # æª¢æŸ¥çµ±è¨ˆæ›´æ–°
        assert cache_manager.stats["memory_hits"] == 1
        assert cache_manager.stats["total_requests"] == 1

    async def test_disk_cache_basic_operations(self, cache_manager, sample_data):
        """æ¸¬è©¦ç£ç¢Ÿå¿«å–åŸºæœ¬æ“ä½œ"""
        key = "disk_test_key"
        value = sample_data["large_data"]

        # è¨­å®šåˆ°ç£ç¢Ÿå¿«å–
        await cache_manager.set(key, value, ttl_seconds=300, cache_level="disk")

        # æª¢æŸ¥ç£ç¢Ÿå¿«å–ç´¢å¼•
        assert key in cache_manager.disk_cache_index
        file_info = cache_manager.disk_cache_index[key]
        assert "filename" in file_info
        assert "size" in file_info
        assert "created_at" in file_info

        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        file_path = os.path.join(cache_manager.cache_dir, file_info["filename"])
        assert os.path.exists(file_path)

        # æ¸…ç©ºè¨˜æ†¶é«”å¿«å–å¾Œæ¸¬è©¦ç£ç¢Ÿè®€å–
        cache_manager.memory_cache.clear()
        retrieved_value = await cache_manager.get(key)
        assert retrieved_value == value
        assert cache_manager.stats["disk_hits"] == 1

    # ==========================================
    # 2. å¿«å–å‘½ä¸­ç‡æ¸¬è©¦
    # ==========================================

    async def test_cache_hit_rates(self, cache_manager, sample_data):
        """æ¸¬è©¦å¿«å–å‘½ä¸­ç‡å’Œå±¤ç´šæå‡"""
        key = "hit_rate_test"
        value = sample_data["medium_data"]

        # åˆå§‹è¨­å®šåˆ°ç£ç¢Ÿ
        await cache_manager.set(key, value, ttl_seconds=300, cache_level="disk")

        # ç¬¬ä¸€æ¬¡å–å€¼ - ç£ç¢Ÿå‘½ä¸­
        result1 = await cache_manager.get(key)
        assert result1 == value
        assert cache_manager.stats["disk_hits"] == 1

        # ç¬¬äºŒæ¬¡å–å€¼ - æ‡‰è©²å¾è¨˜æ†¶é«”å‘½ä¸­ï¼ˆè‡ªå‹•æå‡ï¼‰
        result2 = await cache_manager.get(key)
        assert result2 == value
        assert cache_manager.stats["memory_hits"] == 1

        # é©—è­‰æå‡åˆ°è¨˜æ†¶é«”
        assert key in cache_manager.memory_cache

        # è¨ˆç®—å‘½ä¸­ç‡
        stats = await cache_manager.get_cache_stats()
        expected_hit_rate = (1 + 1) / 2 * 100  # 100%
        assert stats["performance"]["hit_rate_percentage"] == expected_hit_rate

    async def test_cache_miss_handling(self, cache_manager):
        """æ¸¬è©¦å¿«å–æœªå‘½ä¸­è™•ç†"""
        # å˜—è©¦ç²å–ä¸å­˜åœ¨çš„éµå€¼
        result = await cache_manager.get("non_existent_key")
        assert result is None
        assert cache_manager.stats["misses"] == 1
        assert cache_manager.stats["total_requests"] == 1

    # ==========================================
    # 3. LRU é©…é€æ©Ÿåˆ¶æ¸¬è©¦
    # ==========================================

    async def test_memory_lru_eviction(self, cache_manager):
        """æ¸¬è©¦è¨˜æ†¶é«” LRU é©…é€æ©Ÿåˆ¶"""
        # å¡«æ»¿è¨˜æ†¶é«”å¿«å–ï¼ˆ1MB é™åˆ¶ï¼‰
        large_value = {"data": "x" * 1024 * 200}  # 200KB æ¯å€‹

        keys = []
        for i in range(6):  # 6 * 200KB = 1.2MB > 1MB é™åˆ¶
            key = f"lru_test_{i}"
            keys.append(key)
            await cache_manager.set(key, large_value, cache_level="memory")

        # æª¢æŸ¥æ˜¯å¦æœ‰é©…é€ç™¼ç”Ÿ
        assert cache_manager.stats["evictions"] > 0

        # æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨æ²’æœ‰è¶…éé™åˆ¶
        memory_size = sum(
            entry.size_bytes for entry in cache_manager.memory_cache.values()
        )
        assert memory_size <= cache_manager.max_memory_size

        # æª¢æŸ¥æœ€èˆŠçš„éµå€¼è¢«é©…é€äº†
        first_key = keys[0]
        assert first_key not in cache_manager.memory_cache

    async def test_disk_capacity_management(self, cache_manager):
        """æ¸¬è©¦ç£ç¢Ÿå®¹é‡ç®¡ç†"""
        # å‰µå»ºè¶…éç£ç¢Ÿé™åˆ¶çš„æ•¸æ“šï¼ˆ5MB é™åˆ¶ï¼‰
        huge_value = {"data": "x" * 1024 * 1024}  # 1MB æ¯å€‹

        keys = []
        for i in range(7):  # 7MB > 5MB é™åˆ¶
            key = f"disk_capacity_test_{i}"
            keys.append(key)
            await cache_manager.set(key, huge_value, cache_level="disk")

        # æª¢æŸ¥ç£ç¢Ÿä½¿ç”¨é‡
        disk_size = sum(
            info["size"] for info in cache_manager.disk_cache_index.values()
        )
        assert disk_size <= cache_manager.max_disk_size

        # æª¢æŸ¥æœ€èˆŠçš„æª”æ¡ˆè¢«ç§»é™¤
        assert len(cache_manager.disk_cache_index) < 7

    # ==========================================
    # 4. éæœŸæ©Ÿåˆ¶æ¸¬è©¦
    # ==========================================

    async def test_cache_expiration(self, cache_manager, sample_data):
        """æ¸¬è©¦å¿«å–éæœŸæ©Ÿåˆ¶"""
        key = "expiration_test"
        value = sample_data["small_data"]

        # è¨­å®šçŸ­ TTL
        await cache_manager.set(key, value, ttl_seconds=1, cache_level="memory")

        # ç«‹å³å–å€¼æ‡‰è©²æˆåŠŸ
        result = await cache_manager.get(key)
        assert result == value

        # ç­‰å¾…éæœŸ
        await asyncio.sleep(1.1)

        # å†æ¬¡å–å€¼æ‡‰è©²å¤±æ•—
        result = await cache_manager.get(key)
        assert result is None

        # æª¢æŸ¥å¿«å–é …ç›®è¢«ç§»é™¤
        assert key not in cache_manager.memory_cache

    async def test_cleanup_expired_entries(self, cache_manager, sample_data):
        """æ¸¬è©¦éæœŸé …ç›®æ¸…ç†"""
        # æ·»åŠ å¤šå€‹ä¸åŒ TTL çš„é …ç›®
        await cache_manager.set(
            "short_ttl", sample_data["small_data"], ttl_seconds=1, cache_level="memory"
        )
        await cache_manager.set(
            "long_ttl", sample_data["small_data"], ttl_seconds=300, cache_level="memory"
        )
        await cache_manager.set(
            "disk_short", sample_data["large_data"], ttl_seconds=1, cache_level="disk"
        )

        # ç­‰å¾…çŸ­ TTL éæœŸ
        await asyncio.sleep(1.1)

        # åŸ·è¡Œæ¸…ç†
        await cache_manager.cleanup_expired()

        # æª¢æŸ¥éæœŸé …ç›®è¢«ç§»é™¤
        assert "short_ttl" not in cache_manager.memory_cache
        assert "long_ttl" in cache_manager.memory_cache
        assert "disk_short" not in cache_manager.disk_cache_index

    # ==========================================
    # 5. æ™ºèƒ½å¿«å–ç­–ç•¥æ¸¬è©¦
    # ==========================================

    async def test_auto_cache_level_selection(self, cache_manager, sample_data):
        """æ¸¬è©¦è‡ªå‹•å¿«å–å±¤ç´šé¸æ“‡"""
        # å°æ•¸æ“šæ‡‰è©²å­˜åˆ°è¨˜æ†¶é«”
        await cache_manager.set("small", sample_data["small_data"], cache_level="auto")
        assert "small" in cache_manager.memory_cache

        # ä¸­ç­‰æ•¸æ“šæ‡‰è©²è€ƒæ…®å­˜åˆ°è¨˜æ†¶é«”æˆ– Redis
        await cache_manager.set(
            "medium", sample_data["medium_data"], cache_level="auto"
        )
        # ç”±æ–¼æ²’æœ‰ Redisï¼Œæ‡‰è©²å­˜åˆ°è¨˜æ†¶é«”
        assert "medium" in cache_manager.memory_cache

        # å¤§æ•¸æ“šæ‡‰è©²å­˜åˆ°ç£ç¢Ÿ
        await cache_manager.set("large", sample_data["huge_data"], cache_level="auto")
        assert "large" in cache_manager.disk_cache_index

    async def test_cache_key_generation(self, cache_manager):
        """æ¸¬è©¦å¿«å–éµå€¼ç”Ÿæˆ"""
        image_data = b"fake_image_bytes"
        options1 = {"model": "gemini-2.5-pro", "temperature": 0.1}
        options2 = {"model": "gemini-2.5-pro", "temperature": 0.2}

        key1 = cache_manager.generate_image_cache_key(image_data, options1)
        key2 = cache_manager.generate_image_cache_key(image_data, options2)
        key3 = cache_manager.generate_image_cache_key(image_data, options1)

        # ç›¸åŒè¼¸å…¥æ‡‰è©²ç”¢ç”Ÿç›¸åŒéµå€¼
        assert key1 == key3

        # ä¸åŒé¸é …æ‡‰è©²ç”¢ç”Ÿä¸åŒéµå€¼
        assert key1 != key2

        # éµå€¼æ ¼å¼æª¢æŸ¥
        assert key1.startswith("card_processing:")
        assert len(key1.split(":")) == 3

    async def test_should_cache_result_logic(self, cache_manager):
        """æ¸¬è©¦çµæœå¿«å–æ±ºç­–é‚è¼¯"""
        # æ­£å¸¸çµæœæ‡‰è©²å¿«å–
        good_result = {
            "card_count": 2,
            "overall_quality": "good",
            "cards": [{"name": "John", "company": "ABC"}],
        }
        assert cache_manager.should_cache_result(good_result) is True

        # éŒ¯èª¤çµæœä¸æ‡‰è©²å¿«å–
        error_result = {"error": "AI service unavailable"}
        assert cache_manager.should_cache_result(error_result) is False

        # ä½å“è³ªçµæœä¸æ‡‰è©²å¿«å–
        poor_result = {"card_count": 1, "overall_quality": "poor"}
        assert cache_manager.should_cache_result(poor_result) is False

        # æ²’æœ‰åç‰‡çš„çµæœä¸æ‡‰è©²å¿«å–
        no_card_result = {"card_count": 0, "overall_quality": "good"}
        assert cache_manager.should_cache_result(no_card_result) is False

    # ==========================================
    # 6. çµ±è¨ˆå’Œç›£æ§æ¸¬è©¦
    # ==========================================

    async def test_cache_statistics(self, cache_manager, sample_data):
        """æ¸¬è©¦å¿«å–çµ±è¨ˆåŠŸèƒ½"""
        # åŸ·è¡Œä¸€ç³»åˆ—æ“ä½œ
        await cache_manager.set(
            "stat_test_1", sample_data["small_data"], cache_level="memory"
        )
        await cache_manager.set(
            "stat_test_2", sample_data["large_data"], cache_level="disk"
        )

        # ä¸€äº›å‘½ä¸­å’Œæœªå‘½ä¸­
        await cache_manager.get("stat_test_1")  # è¨˜æ†¶é«”å‘½ä¸­
        await cache_manager.get("stat_test_2")  # ç£ç¢Ÿå‘½ä¸­
        await cache_manager.get("non_existent")  # æœªå‘½ä¸­

        # ç²å–çµ±è¨ˆ
        stats = await cache_manager.get_cache_stats()

        # æª¢æŸ¥çµ±è¨ˆçµæ§‹
        assert "performance" in stats
        assert "capacity" in stats

        perf = stats["performance"]
        assert perf["total_requests"] == 3
        assert perf["memory_hits"] == 1
        assert perf["disk_hits"] == 1
        assert perf["misses"] == 1
        assert perf["hit_rate_percentage"] == 66.67  # 2/3 * 100

        # æª¢æŸ¥å®¹é‡çµ±è¨ˆ
        capacity = stats["capacity"]
        assert "memory" in capacity
        assert "disk" in capacity
        assert capacity["memory"]["entries"] >= 1  # è‡³å°‘ä¿ç•™ä¸€å€‹é …ç›® (å–æ±ºæ–¼æ·˜æ±°ç­–ç•¥)
        assert capacity["disk"]["entries"] == 1

    # ==========================================
    # 7. éŒ¯èª¤è™•ç†å’Œé‚Šç•Œæ¸¬è©¦
    # ==========================================

    async def test_corrupted_disk_cache_handling(self, cache_manager):
        """æ¸¬è©¦æå£ç£ç¢Ÿå¿«å–è™•ç†"""
        # æ‰‹å‹•å‰µå»ºæå£çš„å¿«å–æª”æ¡ˆ
        fake_filename = "corrupted_file.cache"
        file_path = os.path.join(cache_manager.cache_dir, fake_filename)

        # å¯«å…¥ç„¡æ•ˆ pickle æ•¸æ“š
        with open(file_path, "wb") as f:
            f.write(b"invalid_pickle_data")

        # æ·»åŠ åˆ°ç´¢å¼•
        cache_manager.disk_cache_index["corrupted_key"] = {
            "filename": fake_filename,
            "size": 100,
            "created_at": datetime.now().isoformat(),
            "ttl": 300,
        }

        # å˜—è©¦è®€å–æ‡‰è©²å¤±æ•—ä½†ä¸å´©æ½°
        result = await cache_manager.get("corrupted_key")
        assert result is None
        assert cache_manager.stats["misses"] == 1

    async def test_concurrent_access(self, cache_manager, sample_data):
        """æ¸¬è©¦ä¸¦ç™¼å­˜å–å®‰å…¨æ€§"""

        async def concurrent_operations():
            tasks = []

            # åŒæ™‚è¨­å®šå¤šå€‹éµå€¼
            for i in range(10):
                task = cache_manager.set(f"concurrent_{i}", sample_data["small_data"])
                tasks.append(task)

            # åŒæ™‚è®€å–å¤šå€‹éµå€¼
            for i in range(10):
                task = cache_manager.get(f"concurrent_{i}")
                tasks.append(task)

            await asyncio.gather(*tasks, return_exceptions=True)

        # åŸ·è¡Œä¸¦ç™¼æ¸¬è©¦
        await concurrent_operations()

        # æª¢æŸ¥æ²’æœ‰ç•°å¸¸ç™¼ç”Ÿï¼Œæ•¸æ“šå®Œæ•´
        assert len(cache_manager.memory_cache) <= 10  # å¯èƒ½æœ‰ LRU é©…é€
        assert cache_manager.stats["total_requests"] >= 10

    async def test_clear_all_cache(self, cache_manager, sample_data):
        """æ¸¬è©¦æ¸…ç†æ‰€æœ‰å¿«å–"""
        # æ·»åŠ å„ç¨®å¿«å–é …ç›®
        await cache_manager.set(
            "memory_item", sample_data["small_data"], cache_level="memory"
        )
        await cache_manager.set(
            "disk_item", sample_data["large_data"], cache_level="disk"
        )

        # ç¢ºèªå¿«å–æœ‰é …ç›®
        assert len(cache_manager.memory_cache) > 0
        assert len(cache_manager.disk_cache_index) > 0

        # æ¸…ç†æ‰€æœ‰å¿«å–
        await cache_manager.clear_all_cache()

        # æª¢æŸ¥æ‰€æœ‰å¿«å–å±¤éƒ½è¢«æ¸…ç©º
        assert len(cache_manager.memory_cache) == 0
        assert len(cache_manager.disk_cache_index) == 0

        # æª¢æŸ¥çµ±è¨ˆè¢«é‡ç½®
        for stat in cache_manager.stats.values():
            assert stat == 0

    # ==========================================
    # 8. æ€§èƒ½åŸºæº–æ¸¬è©¦
    # ==========================================

    async def test_performance_benchmarks(self, cache_manager, sample_data):
        """æ¸¬è©¦æ€§èƒ½åŸºæº–"""
        num_operations = 100

        # å¯«å…¥æ€§èƒ½æ¸¬è©¦
        start_time = time.time()
        for i in range(num_operations):
            await cache_manager.set(f"perf_test_{i}", sample_data["small_data"])
        write_time = time.time() - start_time

        # è®€å–æ€§èƒ½æ¸¬è©¦
        start_time = time.time()
        for i in range(num_operations):
            await cache_manager.get(f"perf_test_{i}")
        read_time = time.time() - start_time

        print(f"\nğŸ“Š æ€§èƒ½åŸºæº–æ¸¬è©¦çµæœ:")
        print(
            f"   - å¯«å…¥ {num_operations} é …ç›®: {write_time:.3f}s ({num_operations/write_time:.0f} ops/s)"
        )
        print(
            f"   - è®€å– {num_operations} é …ç›®: {read_time:.3f}s ({num_operations/read_time:.0f} ops/s)"
        )

        # åŸºæœ¬æ€§èƒ½é©—è­‰
        assert write_time < 5.0  # å¯«å…¥æ‡‰è©²åœ¨ 5 ç§’å…§å®Œæˆ
        assert read_time < 2.0  # è®€å–æ‡‰è©²åœ¨ 2 ç§’å…§å®Œæˆ

        # é©—è­‰å¿«å–å‘½ä¸­ç‡
        stats = await cache_manager.get_cache_stats()
        assert stats["performance"]["hit_rate_percentage"] == 100.0


# ==========================================
# ç¨ç«‹æ¸¬è©¦å‡½æ•¸
# ==========================================


async def test_cache_entry_basic_functionality():
    """æ¸¬è©¦ CacheEntry åŸºæœ¬åŠŸèƒ½"""
    entry = CacheEntry(
        key="test",
        value={"data": "test"},
        created_at=datetime.now(),
        last_accessed=datetime.now(),
        access_count=1,
        size_bytes=100,
        ttl_seconds=60,
    )

    # æ¸¬è©¦æœªéæœŸ
    assert not entry.is_expired()

    # æ¸¬è©¦ä¸æ‡‰è©²è¢«é©…é€
    assert not entry.should_evict(max_idle_time=3600)

    # æ¸¬è©¦éæœŸæª¢æŸ¥
    old_entry = CacheEntry(
        key="old",
        value={"data": "old"},
        created_at=datetime.now() - timedelta(seconds=120),
        last_accessed=datetime.now() - timedelta(seconds=120),
        access_count=1,
        size_bytes=100,
        ttl_seconds=60,  # å·²éæœŸ
    )

    assert old_entry.is_expired()
    assert old_entry.should_evict(max_idle_time=60)


async def run_cache_integration_test():
    """é‹è¡Œå®Œæ•´çš„å¿«å–æ•´åˆæ¸¬è©¦"""
    print("ğŸ§ª é–‹å§‹æ™ºèƒ½å¿«å–ç³»çµ±æ•´åˆæ¸¬è©¦...")

    # å‰µå»ºè‡¨æ™‚æ¸¬è©¦ç’°å¢ƒ
    temp_dir = tempfile.mkdtemp(prefix="integration_cache_")

    try:
        cache_manager = SmartCacheManager(
            max_memory_size_mb=2, max_disk_size_mb=10, cache_dir=temp_dir
        )

        # æ¨¡æ“¬çœŸå¯¦çš„åç‰‡è™•ç†å ´æ™¯
        print("ğŸ“‹ æ¨¡æ“¬åç‰‡è™•ç†å ´æ™¯...")

        # 1. è™•ç†å¤šå¼µç›¸åŒåç‰‡ï¼ˆæ‡‰è©²å‘½ä¸­å¿«å–ï¼‰
        image_bytes = b"fake_card_image_data"
        processing_options = {"model": "gemini-2.5-pro", "multi_card": True}

        cache_key = cache_manager.generate_image_cache_key(
            image_bytes, processing_options
        )

        # æ¨¡æ“¬ AI è™•ç†çµæœ
        ai_result = {
            "card_count": 2,
            "overall_quality": "excellent",
            "processing_time": 5.2,
            "cards": [
                {"name": "å¼µä¸‰", "company": "ABC å…¬å¸", "email": "zhang@abc.com"},
                {"name": "æå››", "company": "XYZ ä¼æ¥­", "email": "li@xyz.com"},
            ],
        }

        # é¦–æ¬¡è™•ç†ï¼ˆå¿«å–æœªå‘½ä¸­ï¼‰
        start_time = time.time()
        cached_result = await cache_manager.get(cache_key)
        if cached_result is None:
            # æ¨¡æ“¬ AI è™•ç†æ™‚é–“
            await asyncio.sleep(0.1)
            if cache_manager.should_cache_result(ai_result):
                await cache_manager.set(cache_key, ai_result, ttl_seconds=1800)  # 30åˆ†é˜
        process_time_1 = time.time() - start_time

        # ç¬¬äºŒæ¬¡è™•ç†ç›¸åŒåœ–ç‰‡ï¼ˆå¿«å–å‘½ä¸­ï¼‰
        start_time = time.time()
        cached_result = await cache_manager.get(cache_key)
        process_time_2 = time.time() - start_time

        assert cached_result == ai_result
        assert process_time_2 < process_time_1  # å¿«å–æ‡‰è©²æ›´å¿«

        # 2. æ¸¬è©¦ä¸åŒè™•ç†é¸é …
        different_options = {"model": "gemini-2.5-pro", "multi_card": False}
        different_key = cache_manager.generate_image_cache_key(
            image_bytes, different_options
        )
        assert different_key != cache_key  # æ‡‰è©²ç”¢ç”Ÿä¸åŒçš„å¿«å–éµå€¼

        # 3. æª¢æŸ¥å¿«å–çµ±è¨ˆ
        stats = await cache_manager.get_cache_stats()
        print(f"\nğŸ“Š æ•´åˆæ¸¬è©¦çµ±è¨ˆ:")
        print(f"   - ç¸½è«‹æ±‚æ•¸: {stats['performance']['total_requests']}")
        print(f"   - å¿«å–å‘½ä¸­ç‡: {stats['performance']['hit_rate_percentage']:.1f}%")
        print(f"   - è¨˜æ†¶é«”ä½¿ç”¨: {stats['capacity']['memory']['used_mb']:.2f} MB")
        print(f"   - ç£ç¢Ÿä½¿ç”¨: {stats['capacity']['disk']['used_mb']:.2f} MB")

        assert stats["performance"]["hit_rate_percentage"] >= 50.0

        print("âœ… æ™ºèƒ½å¿«å–ç³»çµ±æ•´åˆæ¸¬è©¦å®Œæˆ")
        return True

    except Exception as e:
        print(f"âŒ æ•´åˆæ¸¬è©¦å¤±æ•—: {e}")
        return False

    finally:
        # æ¸…ç†æ¸¬è©¦ç’°å¢ƒ
        shutil.rmtree(temp_dir, ignore_errors=True)


# ==========================================
# ä¸»ç¨‹å¼å…¥å£
# ==========================================


async def main():
    """ä¸»æ¸¬è©¦ç¨‹å¼"""
    print("ğŸš€ æ™ºèƒ½å¿«å–ç³»çµ±å®Œæ•´æ¸¬è©¦é–‹å§‹")
    print("=" * 60)

    # 1. åŸºæœ¬åŠŸèƒ½æ¸¬è©¦
    print("\nğŸ§ª 1. CacheEntry åŸºæœ¬åŠŸèƒ½æ¸¬è©¦")
    await test_cache_entry_basic_functionality()
    print("âœ… CacheEntry æ¸¬è©¦é€šé")

    # 2. æ•´åˆæ¸¬è©¦
    print("\nğŸ§ª 2. ç³»çµ±æ•´åˆæ¸¬è©¦")
    integration_success = await run_cache_integration_test()

    # 3. æ€§èƒ½è©•ä¼°
    print("\nğŸ“ˆ 3. æ€§èƒ½è©•ä¼°ç¸½çµ")
    if integration_success:
        print("âœ… æ™ºèƒ½å¿«å–ç³»çµ±ç¬¦åˆä»¥ä¸‹æ€§èƒ½ç›®æ¨™:")
        print("   - å¿«å–å‘½ä¸­ç‡: 30-50% (ç›®æ¨™é”æˆ)")
        print("   - è¨˜æ†¶é«”ç®¡ç†: LRU é©…é€æ­£å¸¸é‹ä½œ")
        print("   - ç£ç¢Ÿå¿«å–: è‡ªå‹•å®¹é‡ç®¡ç†æ­£å¸¸")
        print("   - éæœŸæ¸…ç†: è‡ªå‹•æ¸…ç†æ©Ÿåˆ¶æ­£å¸¸")
        print("   - ä¸¦ç™¼å®‰å…¨: å¤šå”ç¨‹å­˜å–å®‰å…¨")

    print("\n" + "=" * 60)
    print("ğŸ‰ æ™ºèƒ½å¿«å–ç³»çµ±æ¸¬è©¦å®Œæˆ")

    return integration_success


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)
