"""
æ™ºèƒ½å¿«å–ç³»çµ± - AI Engineer å¯¦ä½œ
é‡å° AI åœ–ç‰‡è™•ç†çµæœçš„å¤šå±¤æ¬¡å¿«å–ç­–ç•¥
"""

import asyncio
import hashlib
import io
import json
import os
import pickle
import time
from collections import OrderedDict
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

import aiofiles
import redis.asyncio as redis
from PIL import Image


@dataclass
class CacheEntry:
    """å¿«å–æ¢ç›®"""

    key: str
    value: Any
    created_at: datetime
    last_accessed: datetime
    access_count: int
    size_bytes: int
    ttl_seconds: int

    def is_expired(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦éæœŸ"""
        return datetime.now() > self.created_at + timedelta(seconds=self.ttl_seconds)

    def should_evict(self, max_idle_time: int = 3600) -> bool:
        """æª¢æŸ¥æ˜¯å¦æ‡‰è©²è¢«é©…é€"""
        idle_time = (datetime.now() - self.last_accessed).total_seconds()
        return idle_time > max_idle_time


class SmartCacheManager:
    """æ™ºèƒ½å¿«å–ç®¡ç†å™¨ - å¤šå±¤æ¬¡å¿«å–ç­–ç•¥"""

    def __init__(
        self,
        max_memory_size_mb: int = 100,
        max_disk_size_mb: int = 500,
        redis_url: Optional[str] = None,
        cache_dir: str = "./cache",
    ):
        """
        åˆå§‹åŒ–æ™ºèƒ½å¿«å–ç®¡ç†å™¨

        Args:
            max_memory_size_mb: è¨˜æ†¶é«”å¿«å–æœ€å¤§å¤§å°ï¼ˆMBï¼‰
            max_disk_size_mb: ç£ç¢Ÿå¿«å–æœ€å¤§å¤§å°ï¼ˆMBï¼‰
            redis_url: Redis é€£æ¥ URLï¼ˆå¯é¸ï¼‰
            cache_dir: ç£ç¢Ÿå¿«å–ç›®éŒ„
        """
        self.max_memory_size = max_memory_size_mb * 1024 * 1024  # è½‰æ›ç‚ºä½å…ƒçµ„
        self.max_disk_size = max_disk_size_mb * 1024 * 1024
        self.cache_dir = cache_dir

        # ä¸‰å±¤å¿«å–æ¶æ§‹
        self.memory_cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self.redis_client: Optional[redis.Redis] = None
        self.disk_cache_index: Dict[str, Dict[str, Any]] = {}

        # çµ±è¨ˆè³‡è¨Š
        self.stats = {
            "memory_hits": 0,
            "redis_hits": 0,
            "disk_hits": 0,
            "misses": 0,
            "evictions": 0,
            "total_requests": 0,
        }

        # åˆå§‹åŒ–å¿«å–å±¤
        self._init_cache_layers()

        print(f"âœ… æ™ºèƒ½å¿«å–ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   - è¨˜æ†¶é«”å¿«å–: {max_memory_size_mb} MB")
        print(f"   - ç£ç¢Ÿå¿«å–: {max_disk_size_mb} MB")
        print(f"   - Redis: {'å•Ÿç”¨' if redis_url else 'åœç”¨'}")

    def _init_cache_layers(self):
        """åˆå§‹åŒ–å¿«å–å±¤"""
        # å»ºç«‹ç£ç¢Ÿå¿«å–ç›®éŒ„
        os.makedirs(self.cache_dir, exist_ok=True)

        # è¼‰å…¥ç£ç¢Ÿå¿«å–ç´¢å¼•
        asyncio.create_task(self._load_disk_cache_index())

        # å˜—è©¦é€£æ¥ Redis
        redis_url = os.getenv("REDIS_URL")
        if redis_url:
            try:
                self.redis_client = redis.from_url(redis_url)
                print("âœ… Redis å¿«å–å±¤å·²å•Ÿç”¨")
            except Exception as e:
                print(f"âš ï¸ Redis é€£æ¥å¤±æ•—: {e}")

    async def get(self, key: str) -> Optional[Any]:
        """
        ç²å–å¿«å–å€¼ - æ™ºèƒ½ä¸‰å±¤å¿«å–æŸ¥æ‰¾

        Args:
            key: å¿«å–éµå€¼

        Returns:
            å¿«å–å€¼æˆ– None
        """
        self.stats["total_requests"] += 1

        # å±¤ç´š 1ï¼šè¨˜æ†¶é«”å¿«å–ï¼ˆæœ€å¿«ï¼‰
        if key in self.memory_cache:
            entry = self.memory_cache[key]
            if not entry.is_expired():
                entry.last_accessed = datetime.now()
                entry.access_count += 1
                # ç§»åˆ°æœ€å‰é¢ï¼ˆLRUï¼‰
                self.memory_cache.move_to_end(key, last=False)
                self.stats["memory_hits"] += 1
                return entry.value
            else:
                # éæœŸäº†ï¼Œç§»é™¤
                del self.memory_cache[key]

        # å±¤ç´š 2ï¼šRedis å¿«å–ï¼ˆä¸­ç­‰é€Ÿåº¦ï¼‰
        if self.redis_client:
            try:
                cached_data = await self.redis_client.get(key)
                if cached_data:
                    value = pickle.loads(cached_data)
                    # æå‡åˆ°è¨˜æ†¶é«”å¿«å–
                    await self._promote_to_memory(key, value, ttl_seconds=3600)
                    self.stats["redis_hits"] += 1
                    return value
            except Exception as e:
                print(f"âš ï¸ Redis è®€å–éŒ¯èª¤: {e}")

        # å±¤ç´š 3ï¼šç£ç¢Ÿå¿«å–ï¼ˆè¼ƒæ…¢ä½†å®¹é‡å¤§ï¼‰
        if key in self.disk_cache_index:
            file_info = self.disk_cache_index[key]
            file_path = os.path.join(self.cache_dir, file_info["filename"])

            if os.path.exists(file_path):
                try:
                    # æª¢æŸ¥æ˜¯å¦éæœŸ
                    created_at = datetime.fromisoformat(file_info["created_at"])
                    if datetime.now() < created_at + timedelta(
                        seconds=file_info["ttl"]
                    ):
                        async with aiofiles.open(file_path, "rb") as f:
                            content = await f.read()
                            value = pickle.loads(content)

                        # æå‡åˆ°ä¸Šå±¤å¿«å–
                        await self._promote_to_redis(key, value, file_info["ttl"])
                        await self._promote_to_memory(key, value, file_info["ttl"])

                        self.stats["disk_hits"] += 1
                        return value
                    else:
                        # éæœŸï¼Œæ¸…ç†
                        await self._remove_from_disk(key)
                except Exception as e:
                    print(f"âš ï¸ ç£ç¢Ÿå¿«å–è®€å–éŒ¯èª¤: {e}")

        self.stats["misses"] += 1
        return None

    async def set(
        self, key: str, value: Any, ttl_seconds: int = 3600, cache_level: str = "auto"
    ):
        """
        è¨­å®šå¿«å–å€¼ - æ™ºèƒ½å­˜å„²ç­–ç•¥

        Args:
            key: å¿«å–éµå€¼
            value: å¿«å–å€¼
            ttl_seconds: ç”Ÿå­˜æ™‚é–“ï¼ˆç§’ï¼‰
            cache_level: å¿«å–å±¤ç´š (auto/memory/redis/disk)
        """
        # è¨ˆç®—å€¼çš„å¤§å°
        value_size = len(pickle.dumps(value))

        if cache_level == "auto":
            # è‡ªå‹•æ±ºå®šæœ€é©åˆçš„å¿«å–å±¤ç´š
            if value_size < 1024 * 50:  # < 50KBï¼Œå­˜åˆ°è¨˜æ†¶é«”
                cache_level = "memory"
            elif value_size < 1024 * 500:  # < 500KBï¼Œå­˜åˆ° Redis
                cache_level = "redis"
            else:  # >= 500KBï¼Œå­˜åˆ°ç£ç¢Ÿ
                cache_level = "disk"

        # æ ¹æ“šç­–ç•¥å­˜å„²
        if cache_level == "memory" or cache_level == "auto":
            await self._set_memory(key, value, ttl_seconds, value_size)

        if cache_level == "redis" and self.redis_client:
            await self._set_redis(key, value, ttl_seconds)

        if cache_level == "disk":
            await self._set_disk(key, value, ttl_seconds, value_size)

    async def _set_memory(self, key: str, value: Any, ttl: int, size: int):
        """è¨­å®šè¨˜æ†¶é«”å¿«å–"""
        # æª¢æŸ¥è¨˜æ†¶é«”å®¹é‡
        await self._ensure_memory_capacity(size)

        entry = CacheEntry(
            key=key,
            value=value,
            created_at=datetime.now(),
            last_accessed=datetime.now(),
            access_count=1,
            size_bytes=size,
            ttl_seconds=ttl,
        )

        self.memory_cache[key] = entry
        # æ–°é …ç›®æ”¾åˆ°æœ€å‰é¢
        self.memory_cache.move_to_end(key, last=False)

    async def _set_redis(self, key: str, value: Any, ttl: int):
        """è¨­å®š Redis å¿«å–"""
        if not self.redis_client:
            return

        try:
            serialized = pickle.dumps(value)
            await self.redis_client.setex(key, ttl, serialized)
        except Exception as e:
            print(f"âš ï¸ Redis å¯«å…¥éŒ¯èª¤: {e}")

    async def _set_disk(self, key: str, value: Any, ttl: int, size: int):
        """è¨­å®šç£ç¢Ÿå¿«å–"""
        # æª¢æŸ¥ç£ç¢Ÿå®¹é‡
        await self._ensure_disk_capacity(size)

        filename = f"{hashlib.md5(key.encode()).hexdigest()}.cache"
        file_path = os.path.join(self.cache_dir, filename)

        try:
            serialized = pickle.dumps(value)
            async with aiofiles.open(file_path, "wb") as f:
                await f.write(serialized)

            # æ›´æ–°ç´¢å¼•
            self.disk_cache_index[key] = {
                "filename": filename,
                "size": size,
                "created_at": datetime.now().isoformat(),
                "ttl": ttl,
            }

            await self._save_disk_cache_index()

        except Exception as e:
            print(f"âš ï¸ ç£ç¢Ÿå¿«å–å¯«å…¥éŒ¯èª¤: {e}")

    async def _promote_to_memory(self, key: str, value: Any, ttl_seconds: int):
        """æå‡åˆ°è¨˜æ†¶é«”å¿«å–"""
        value_size = len(pickle.dumps(value))
        if value_size < 1024 * 100:  # åªæœ‰å°æ–¼ 100KB çš„æ‰æå‡åˆ°è¨˜æ†¶é«”
            await self._set_memory(key, value, ttl_seconds, value_size)

    async def _promote_to_redis(self, key: str, value: Any, ttl_seconds: int):
        """æå‡åˆ° Redis å¿«å–"""
        if self.redis_client:
            await self._set_redis(key, value, ttl_seconds)

    async def _ensure_memory_capacity(self, needed_size: int):
        """ç¢ºä¿è¨˜æ†¶é«”å®¹é‡è¶³å¤ """
        current_size = sum(entry.size_bytes for entry in self.memory_cache.values())

        while current_size + needed_size > self.max_memory_size and self.memory_cache:
            # LRU é©…é€ï¼šç§»é™¤æœ€ä¹…æœªä½¿ç”¨çš„é …ç›®
            oldest_key = next(reversed(self.memory_cache))
            evicted_entry = self.memory_cache.pop(oldest_key)
            current_size -= evicted_entry.size_bytes
            self.stats["evictions"] += 1

    async def _ensure_disk_capacity(self, needed_size: int):
        """ç¢ºä¿ç£ç¢Ÿå®¹é‡è¶³å¤ """
        current_size = sum(info["size"] for info in self.disk_cache_index.values())

        while current_size + needed_size > self.max_disk_size and self.disk_cache_index:
            # æ‰¾åˆ°æœ€èˆŠçš„æª”æ¡ˆä¸¦ç§»é™¤
            oldest_key = min(
                self.disk_cache_index.keys(),
                key=lambda k: self.disk_cache_index[k]["created_at"],
            )
            await self._remove_from_disk(oldest_key)
            current_size = sum(info["size"] for info in self.disk_cache_index.values())

    async def _remove_from_disk(self, key: str):
        """å¾ç£ç¢Ÿç§»é™¤å¿«å–é …ç›®"""
        if key not in self.disk_cache_index:
            return

        file_info = self.disk_cache_index[key]
        file_path = os.path.join(self.cache_dir, file_info["filename"])

        try:
            if os.path.exists(file_path):
                os.remove(file_path)
            del self.disk_cache_index[key]
            await self._save_disk_cache_index()
        except Exception as e:
            print(f"âš ï¸ ç§»é™¤ç£ç¢Ÿå¿«å–éŒ¯èª¤: {e}")

    async def _load_disk_cache_index(self):
        """è¼‰å…¥ç£ç¢Ÿå¿«å–ç´¢å¼•"""
        index_file = os.path.join(self.cache_dir, "cache_index.json")
        try:
            if os.path.exists(index_file):
                async with aiofiles.open(index_file, "r", encoding="utf-8") as f:
                    content = await f.read()
                    self.disk_cache_index = json.loads(content)
                print(f"âœ… è¼‰å…¥ç£ç¢Ÿå¿«å–ç´¢å¼•: {len(self.disk_cache_index)} é …ç›®")
        except Exception as e:
            print(f"âš ï¸ è¼‰å…¥ç£ç¢Ÿå¿«å–ç´¢å¼•å¤±æ•—: {e}")
            self.disk_cache_index = {}

    async def _save_disk_cache_index(self):
        """å„²å­˜ç£ç¢Ÿå¿«å–ç´¢å¼•"""
        index_file = os.path.join(self.cache_dir, "cache_index.json")
        try:
            async with aiofiles.open(index_file, "w", encoding="utf-8") as f:
                await f.write(
                    json.dumps(self.disk_cache_index, indent=2, ensure_ascii=False)
                )
        except Exception as e:
            print(f"âš ï¸ å„²å­˜ç£ç¢Ÿå¿«å–ç´¢å¼•å¤±æ•—: {e}")

    def generate_image_cache_key(
        self, image_bytes: bytes, processing_options: Dict[str, Any] = None
    ) -> str:
        """
        ç”Ÿæˆåœ–ç‰‡å¿«å–éµå€¼ - è€ƒæ…®åœ–ç‰‡å…§å®¹å’Œè™•ç†é¸é …

        Args:
            image_bytes: åœ–ç‰‡ä½å…ƒçµ„è³‡æ–™
            processing_options: è™•ç†é¸é …

        Returns:
            å¿«å–éµå€¼
        """
        # åœ–ç‰‡å…§å®¹é›œæ¹Š
        image_hash = hashlib.md5(image_bytes).hexdigest()

        # è™•ç†é¸é …é›œæ¹Š
        options_str = json.dumps(processing_options or {}, sort_keys=True)
        options_hash = hashlib.md5(options_str.encode()).hexdigest()

        return f"card_processing:{image_hash}:{options_hash}"

    def should_cache_result(self, result: Dict[str, Any]) -> bool:
        """
        åˆ¤æ–·çµæœæ˜¯å¦æ‡‰è©²è¢«å¿«å–

        Args:
            result: è™•ç†çµæœ

        Returns:
            æ˜¯å¦æ‡‰è©²å¿«å–
        """
        # ä¸å¿«å–éŒ¯èª¤çµæœ
        if "error" in result:
            return False

        # ä¸å¿«å–ä½å“è³ªçµæœ
        if result.get("overall_quality") == "poor":
            return False

        # ä¸å¿«å–æ²’æœ‰å¡ç‰‡çš„çµæœ
        if result.get("card_count", 0) == 0:
            return False

        return True

    async def get_cache_stats(self) -> Dict[str, Any]:
        """ç²å–å¿«å–çµ±è¨ˆ"""
        memory_size = sum(entry.size_bytes for entry in self.memory_cache.values())
        disk_size = sum(info["size"] for info in self.disk_cache_index.values())

        # Redis çµ±è¨ˆ
        redis_stats = {}
        if self.redis_client:
            try:
                redis_info = await self.redis_client.info("memory")
                redis_stats = {
                    "used_memory": redis_info.get("used_memory", 0),
                    "used_memory_human": redis_info.get("used_memory_human", "N/A"),
                }
            except:
                redis_stats = {"error": "Unable to get Redis stats"}

        total_requests = self.stats["total_requests"]
        hit_rate = (
            (
                self.stats["memory_hits"]
                + self.stats["redis_hits"]
                + self.stats["disk_hits"]
            )
            / max(total_requests, 1)
            * 100
        )

        return {
            "performance": {
                "total_requests": total_requests,
                "hit_rate_percentage": round(hit_rate, 2),
                "memory_hits": self.stats["memory_hits"],
                "redis_hits": self.stats["redis_hits"],
                "disk_hits": self.stats["disk_hits"],
                "misses": self.stats["misses"],
                "evictions": self.stats["evictions"],
            },
            "capacity": {
                "memory": {
                    "used_bytes": memory_size,
                    "used_mb": round(memory_size / 1024 / 1024, 2),
                    "max_mb": round(self.max_memory_size / 1024 / 1024),
                    "usage_percentage": round(
                        memory_size / self.max_memory_size * 100, 2
                    ),
                    "entries": len(self.memory_cache),
                },
                "disk": {
                    "used_bytes": disk_size,
                    "used_mb": round(disk_size / 1024 / 1024, 2),
                    "max_mb": round(self.max_disk_size / 1024 / 1024),
                    "usage_percentage": round(disk_size / self.max_disk_size * 100, 2),
                    "entries": len(self.disk_cache_index),
                },
                "redis": redis_stats,
            },
        }

    async def cleanup_expired(self):
        """æ¸…ç†éæœŸå¿«å–é …ç›®"""
        # æ¸…ç†è¨˜æ†¶é«”å¿«å–
        expired_keys = [
            key
            for key, entry in self.memory_cache.items()
            if entry.is_expired() or entry.should_evict()
        ]
        for key in expired_keys:
            del self.memory_cache[key]

        # æ¸…ç†ç£ç¢Ÿå¿«å–
        disk_expired = []
        for key, info in self.disk_cache_index.items():
            created_at = datetime.fromisoformat(info["created_at"])
            if datetime.now() > created_at + timedelta(seconds=info["ttl"]):
                disk_expired.append(key)

        for key in disk_expired:
            await self._remove_from_disk(key)

        if expired_keys or disk_expired:
            print(
                f"ğŸ§¹ æ¸…ç†éæœŸå¿«å–: è¨˜æ†¶é«” {len(expired_keys)} é …ï¼Œç£ç¢Ÿ {len(disk_expired)} é …"
            )

    async def clear_all_cache(self):
        """æ¸…ç†æ‰€æœ‰å¿«å–"""
        # æ¸…ç†è¨˜æ†¶é«”
        self.memory_cache.clear()

        # æ¸…ç† Redis
        if self.redis_client:
            try:
                # åªæ¸…ç†æˆ‘å€‘çš„ keysï¼ˆä»¥ card_processing: é–‹é ­ï¼‰
                keys = await self.redis_client.keys("card_processing:*")
                if keys:
                    await self.redis_client.delete(*keys)
            except Exception as e:
                print(f"âš ï¸ æ¸…ç† Redis å¿«å–éŒ¯èª¤: {e}")

        # æ¸…ç†ç£ç¢Ÿ
        for key in list(self.disk_cache_index.keys()):
            await self._remove_from_disk(key)

        # é‡ç½®çµ±è¨ˆ
        self.stats = {k: 0 for k in self.stats}

        print("ğŸ§¹ å·²æ¸…ç†æ‰€æœ‰å¿«å–å±¤")
