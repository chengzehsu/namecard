"""
AI è™•ç†æ•ˆèƒ½ç›£æ§å™¨ - AI Engineer å¯¦ä½œ
ç›£æ§å’Œåˆ†æ AI è™•ç†ç®¡ç·šçš„æ•ˆèƒ½æŒ‡æ¨™
"""

import asyncio
import json
import statistics
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

import aiofiles

from simple_config import Config


@dataclass
class ProcessingMetrics:
    """å–®æ¬¡è™•ç†æŒ‡æ¨™"""

    request_id: str
    start_time: float
    end_time: float
    processing_time: float
    api_key_used: str
    image_size_bytes: int
    result_quality: str  # good/partial/poor
    card_count: int
    success: bool
    error_message: Optional[str] = None
    cache_hit: bool = False
    retry_count: int = 0

    @property
    def processing_time_ms(self) -> float:
        return self.processing_time * 1000


@dataclass
class SystemHealth:
    """ç³»çµ±å¥åº·ç‹€æ…‹"""

    timestamp: datetime
    cpu_usage: float
    memory_usage: float
    active_connections: int
    queue_size: int
    avg_response_time: float
    error_rate: float
    throughput: float  # requests per second


class PerformanceMonitor:
    """AI è™•ç†æ•ˆèƒ½ç›£æ§å™¨"""

    def __init__(self, max_history_size: int = 10000):
        """
        åˆå§‹åŒ–æ•ˆèƒ½ç›£æ§å™¨

        Args:
            max_history_size: æœ€å¤§æ­·å²è¨˜éŒ„æ•¸é‡
        """
        self.max_history_size = max_history_size

        # æ­·å²è¨˜éŒ„
        self.processing_history: deque = deque(maxlen=max_history_size)
        self.health_history: deque = deque(maxlen=1000)  # ä¿ç•™æœ€è¿‘1000å€‹å¥åº·æª¢æŸ¥

        # å¯¦æ™‚çµ±è¨ˆ
        self.real_time_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_processing_time": 0.0,
            "cache_hits": 0,
            "current_hour_requests": 0,
            "current_minute_requests": 0,
        }

        # åˆ†æ™‚çµ±è¨ˆ
        self.hourly_stats = defaultdict(
            lambda: {"requests": 0, "avg_time": 0.0, "error_rate": 0.0}
        )
        self.minute_stats = deque(maxlen=60)  # æœ€è¿‘60åˆ†é˜

        # æ•ˆèƒ½é–¾å€¼
        self.performance_thresholds = {
            "max_response_time_ms": 15000,  # 15ç§’
            "max_error_rate": 0.05,  # 5%
            "max_queue_size": 100,
            "min_success_rate": 0.95,  # 95%
        }

        # ç•°å¸¸æª¢æ¸¬
        self.anomaly_detection = {
            "response_time_window": deque(maxlen=100),
            "error_rate_window": deque(maxlen=100),
            "throughput_window": deque(maxlen=60),
        }

        # å•Ÿå‹•èƒŒæ™¯ä»»å‹™
        self._start_background_monitoring()

        print("âœ… AI æ•ˆèƒ½ç›£æ§å™¨åˆå§‹åŒ–å®Œæˆ")

    async def record_processing(
        self,
        request_id: str,
        start_time: float,
        end_time: float,
        api_key_used: str,
        image_size_bytes: int,
        result: Dict[str, Any],
        cache_hit: bool = False,
        retry_count: int = 0,
    ):
        """
        è¨˜éŒ„è™•ç†æŒ‡æ¨™

        Args:
            request_id: è«‹æ±‚ ID
            start_time: é–‹å§‹æ™‚é–“æˆ³
            end_time: çµæŸæ™‚é–“æˆ³
            api_key_used: ä½¿ç”¨çš„ API Key
            image_size_bytes: åœ–ç‰‡å¤§å°
            result: è™•ç†çµæœ
            cache_hit: æ˜¯å¦å‘½ä¸­å¿«å–
            retry_count: é‡è©¦æ¬¡æ•¸
        """
        processing_time = end_time - start_time
        success = "error" not in result

        metrics = ProcessingMetrics(
            request_id=request_id,
            start_time=start_time,
            end_time=end_time,
            processing_time=processing_time,
            api_key_used=api_key_used,
            image_size_bytes=image_size_bytes,
            result_quality=result.get("overall_quality", "unknown"),
            card_count=result.get("card_count", 0),
            success=success,
            error_message=result.get("error"),
            cache_hit=cache_hit,
            retry_count=retry_count,
        )

        # åŠ å…¥æ­·å²è¨˜éŒ„
        self.processing_history.append(metrics)

        # æ›´æ–°å¯¦æ™‚çµ±è¨ˆ
        self._update_real_time_stats(metrics)

        # ç•°å¸¸æª¢æ¸¬
        await self._detect_anomalies(metrics)

        # è¨˜éŒ„åˆ°æª”æ¡ˆï¼ˆéé˜»å¡ï¼‰
        asyncio.create_task(self._persist_metrics(metrics))

    def _update_real_time_stats(self, metrics: ProcessingMetrics):
        """æ›´æ–°å¯¦æ™‚çµ±è¨ˆ"""
        self.real_time_stats["total_requests"] += 1

        if metrics.success:
            self.real_time_stats["successful_requests"] += 1
        else:
            self.real_time_stats["failed_requests"] += 1

        self.real_time_stats["total_processing_time"] += metrics.processing_time

        if metrics.cache_hit:
            self.real_time_stats["cache_hits"] += 1

        # åˆ†æ™‚çµ±è¨ˆ
        current_hour = datetime.now().hour
        self.hourly_stats[current_hour]["requests"] += 1

        # æ›´æ–°æ¯å°æ™‚å¹³å‡æ™‚é–“
        hour_stats = self.hourly_stats[current_hour]
        total_requests = hour_stats["requests"]
        current_avg = hour_stats["avg_time"]
        hour_stats["avg_time"] = (
            current_avg * (total_requests - 1) + metrics.processing_time
        ) / total_requests

    async def _detect_anomalies(self, metrics: ProcessingMetrics):
        """æª¢æ¸¬ç•°å¸¸æƒ…æ³"""
        # å›æ‡‰æ™‚é–“ç•°å¸¸
        self.anomaly_detection["response_time_window"].append(metrics.processing_time)
        if len(self.anomaly_detection["response_time_window"]) >= 10:
            recent_times = list(self.anomaly_detection["response_time_window"])
            avg_time = statistics.mean(recent_times)
            std_dev = statistics.stdev(recent_times) if len(recent_times) > 1 else 0

            # æª¢æ¸¬æ˜¯å¦è¶…é 2 å€‹æ¨™æº–å·®
            if metrics.processing_time > avg_time + (2 * std_dev):
                await self._log_anomaly(
                    "response_time_spike",
                    f"è™•ç†æ™‚é–“ç•°å¸¸: {metrics.processing_time:.2f}s (å¹³å‡: {avg_time:.2f}s)",
                )

        # éŒ¯èª¤ç‡ç•°å¸¸
        error_rate = self._calculate_recent_error_rate()
        if error_rate > self.performance_thresholds["max_error_rate"]:
            await self._log_anomaly("high_error_rate", f"éŒ¯èª¤ç‡éé«˜: {error_rate:.2%}")

    async def _log_anomaly(self, anomaly_type: str, message: str):
        """è¨˜éŒ„ç•°å¸¸"""
        anomaly = {
            "type": anomaly_type,
            "message": message,
            "timestamp": datetime.now().isoformat(),
            "recent_metrics": self._get_recent_summary(),
        }

        print(f"ğŸš¨ æ•ˆèƒ½ç•°å¸¸æª¢æ¸¬: {message}")

        # å¯ä»¥åœ¨é€™è£¡æ·»åŠ è­¦å ±é€šçŸ¥é‚è¼¯
        # await self._send_alert(anomaly)

    def get_performance_summary(self, time_range_minutes: int = 60) -> Dict[str, Any]:
        """
        ç²å–æ•ˆèƒ½æ‘˜è¦

        Args:
            time_range_minutes: æ™‚é–“ç¯„åœï¼ˆåˆ†é˜ï¼‰

        Returns:
            æ•ˆèƒ½æ‘˜è¦
        """
        cutoff_time = time.time() - (time_range_minutes * 60)
        recent_metrics = [
            m for m in self.processing_history if m.start_time > cutoff_time
        ]

        if not recent_metrics:
            return {"message": "No data in specified time range"}

        # åŸºæœ¬çµ±è¨ˆ
        total_requests = len(recent_metrics)
        successful_requests = sum(1 for m in recent_metrics if m.success)
        failed_requests = total_requests - successful_requests

        # æ™‚é–“çµ±è¨ˆ
        processing_times = [m.processing_time for m in recent_metrics]
        avg_processing_time = statistics.mean(processing_times)
        p95_processing_time = (
            statistics.quantiles(processing_times, n=20)[18]  # 95th percentile
            if len(processing_times) > 10
            else avg_processing_time
        )
        p99_processing_time = (
            statistics.quantiles(processing_times, n=100)[98]  # 99th percentile
            if len(processing_times) > 100
            else avg_processing_time
        )

        # å“è³ªçµ±è¨ˆ
        quality_distribution = defaultdict(int)
        for m in recent_metrics:
            quality_distribution[m.result_quality] += 1

        # å¿«å–çµ±è¨ˆ
        cache_hits = sum(1 for m in recent_metrics if m.cache_hit)
        cache_hit_rate = cache_hits / total_requests if total_requests > 0 else 0

        # API Key ä½¿ç”¨åˆ†ä½ˆ
        api_key_usage = defaultdict(int)
        for m in recent_metrics:
            api_key_usage[m.api_key_used] += 1

        # ååé‡è¨ˆç®—
        time_span_hours = time_range_minutes / 60
        throughput_per_hour = (
            total_requests / time_span_hours if time_span_hours > 0 else 0
        )

        return {
            "time_range_minutes": time_range_minutes,
            "summary": {
                "total_requests": total_requests,
                "successful_requests": successful_requests,
                "failed_requests": failed_requests,
                "success_rate": (
                    successful_requests / total_requests if total_requests > 0 else 0
                ),
                "throughput_per_hour": round(throughput_per_hour, 2),
            },
            "performance": {
                "avg_processing_time_ms": round(avg_processing_time * 1000, 2),
                "p95_processing_time_ms": round(p95_processing_time * 1000, 2),
                "p99_processing_time_ms": round(p99_processing_time * 1000, 2),
                "min_processing_time_ms": round(min(processing_times) * 1000, 2),
                "max_processing_time_ms": round(max(processing_times) * 1000, 2),
            },
            "quality_distribution": dict(quality_distribution),
            "cache_performance": {
                "cache_hits": cache_hits,
                "cache_hit_rate": round(cache_hit_rate, 4),
                "cache_miss_rate": round(1 - cache_hit_rate, 4),
            },
            "api_key_usage": dict(api_key_usage),
            "health_status": self._assess_health_status(recent_metrics),
        }

    def _assess_health_status(
        self, recent_metrics: List[ProcessingMetrics]
    ) -> Dict[str, Any]:
        """è©•ä¼°ç³»çµ±å¥åº·ç‹€æ…‹"""
        if not recent_metrics:
            return {"status": "unknown", "reason": "No recent data"}

        total_requests = len(recent_metrics)
        successful_requests = sum(1 for m in recent_metrics if m.success)
        success_rate = successful_requests / total_requests

        avg_response_time = statistics.mean(m.processing_time for m in recent_metrics)

        # å¥åº·ç‹€æ…‹åˆ¤æ–·
        issues = []

        if success_rate < self.performance_thresholds["min_success_rate"]:
            issues.append(f"æˆåŠŸç‡ä½: {success_rate:.2%}")

        if avg_response_time > (
            self.performance_thresholds["max_response_time_ms"] / 1000
        ):
            issues.append(f"å›æ‡‰æ™‚é–“éé•·: {avg_response_time:.2f}s")

        error_rate = 1 - success_rate
        if error_rate > self.performance_thresholds["max_error_rate"]:
            issues.append(f"éŒ¯èª¤ç‡éé«˜: {error_rate:.2%}")

        if not issues:
            return {
                "status": "healthy",
                "success_rate": round(success_rate, 4),
                "avg_response_time_ms": round(avg_response_time * 1000, 2),
            }
        else:
            return {
                "status": "degraded" if len(issues) < 3 else "unhealthy",
                "issues": issues,
                "success_rate": round(success_rate, 4),
                "avg_response_time_ms": round(avg_response_time * 1000, 2),
            }

    def _calculate_recent_error_rate(self, window_size: int = 50) -> float:
        """è¨ˆç®—æœ€è¿‘çš„éŒ¯èª¤ç‡"""
        if len(self.processing_history) < window_size:
            recent_metrics = list(self.processing_history)
        else:
            recent_metrics = list(self.processing_history)[-window_size:]

        if not recent_metrics:
            return 0.0

        failed_count = sum(1 for m in recent_metrics if not m.success)
        return failed_count / len(recent_metrics)

    def _get_recent_summary(self, count: int = 10) -> Dict[str, Any]:
        """ç²å–æœ€è¿‘è™•ç†çš„æ‘˜è¦"""
        recent = (
            list(self.processing_history)[-count:] if self.processing_history else []
        )

        return {
            "count": len(recent),
            "avg_time_ms": (
                round(statistics.mean(m.processing_time for m in recent) * 1000, 2)
                if recent
                else 0
            ),
            "success_rate": (
                sum(1 for m in recent if m.success) / len(recent) if recent else 0
            ),
            "cache_hit_rate": (
                sum(1 for m in recent if m.cache_hit) / len(recent) if recent else 0
            ),
        }

    async def generate_performance_report(
        self, hours: int = 24, include_raw_data: bool = False
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆè©³ç´°æ•ˆèƒ½å ±å‘Š

        Args:
            hours: å ±å‘Šæ™‚é–“ç¯„åœï¼ˆå°æ™‚ï¼‰
            include_raw_data: æ˜¯å¦åŒ…å«åŸå§‹è³‡æ–™

        Returns:
            è©³ç´°æ•ˆèƒ½å ±å‘Š
        """
        cutoff_time = time.time() - (hours * 3600)
        relevant_metrics = [
            m for m in self.processing_history if m.start_time > cutoff_time
        ]

        if not relevant_metrics:
            return {"error": "No data available for the specified time range"}

        # æŒ‰å°æ™‚åˆ†çµ„çµ±è¨ˆ
        hourly_breakdown = defaultdict(
            lambda: {"requests": 0, "success_count": 0, "total_time": 0, "errors": []}
        )

        for metric in relevant_metrics:
            hour_key = datetime.fromtimestamp(metric.start_time).strftime(
                "%Y-%m-%d %H:00"
            )
            hour_data = hourly_breakdown[hour_key]

            hour_data["requests"] += 1
            hour_data["total_time"] += metric.processing_time

            if metric.success:
                hour_data["success_count"] += 1
            else:
                hour_data["errors"].append(metric.error_message)

        # è¨ˆç®—æ¯å°æ™‚çµ±è¨ˆ
        hourly_stats = {}
        for hour, data in hourly_breakdown.items():
            hourly_stats[hour] = {
                "requests": data["requests"],
                "success_rate": data["success_count"] / data["requests"],
                "avg_response_time_ms": round(
                    (data["total_time"] / data["requests"]) * 1000, 2
                ),
                "error_count": len(data["errors"]),
                "unique_errors": len(set(data["errors"])),
            }

        # é ‚å±¤çµ±è¨ˆ
        total_requests = len(relevant_metrics)
        total_success = sum(1 for m in relevant_metrics if m.success)

        # æ•ˆèƒ½è¶¨å‹¢åˆ†æ
        processing_times = [m.processing_time for m in relevant_metrics]

        report = {
            "report_metadata": {
                "generated_at": datetime.now().isoformat(),
                "time_range_hours": hours,
                "total_requests_analyzed": total_requests,
            },
            "executive_summary": {
                "overall_success_rate": round(total_success / total_requests, 4),
                "avg_processing_time_ms": round(
                    statistics.mean(processing_times) * 1000, 2
                ),
                "total_processing_hours": round(sum(processing_times) / 3600, 2),
                "peak_hour": max(hourly_stats.items(), key=lambda x: x[1]["requests"]),
                "slowest_hour": max(
                    hourly_stats.items(), key=lambda x: x[1]["avg_response_time_ms"]
                ),
            },
            "performance_breakdown": {
                "response_time_percentiles": {
                    "p50": round(statistics.median(processing_times) * 1000, 2),
                    "p90": (
                        round(statistics.quantiles(processing_times, n=10)[8] * 1000, 2)
                        if len(processing_times) > 10
                        else 0
                    ),
                    "p95": (
                        round(
                            statistics.quantiles(processing_times, n=20)[18] * 1000, 2
                        )
                        if len(processing_times) > 20
                        else 0
                    ),
                    "p99": (
                        round(
                            statistics.quantiles(processing_times, n=100)[98] * 1000, 2
                        )
                        if len(processing_times) > 100
                        else 0
                    ),
                },
                "quality_metrics": {
                    quality: sum(
                        1 for m in relevant_metrics if m.result_quality == quality
                    )
                    for quality in set(m.result_quality for m in relevant_metrics)
                },
                "cache_efficiency": {
                    "total_cache_hits": sum(1 for m in relevant_metrics if m.cache_hit),
                    "cache_hit_rate": round(
                        sum(1 for m in relevant_metrics if m.cache_hit)
                        / total_requests,
                        4,
                    ),
                },
            },
            "hourly_breakdown": hourly_stats,
            "recommendations": self._generate_recommendations(relevant_metrics),
        }

        if include_raw_data:
            report["raw_metrics"] = [
                {
                    "request_id": m.request_id,
                    "timestamp": datetime.fromtimestamp(m.start_time).isoformat(),
                    "processing_time_ms": m.processing_time_ms,
                    "result_quality": m.result_quality,
                    "success": m.success,
                    "cache_hit": m.cache_hit,
                    "retry_count": m.retry_count,
                }
                for m in relevant_metrics
            ]

        return report

    def _generate_recommendations(self, metrics: List[ProcessingMetrics]) -> List[str]:
        """åŸºæ–¼æ•ˆèƒ½è³‡æ–™ç”Ÿæˆå»ºè­°"""
        recommendations = []

        if not metrics:
            return recommendations

        # åˆ†æè™•ç†æ™‚é–“
        processing_times = [m.processing_time for m in metrics]
        avg_time = statistics.mean(processing_times)
        p95_time = (
            statistics.quantiles(processing_times, n=20)[18]
            if len(processing_times) > 20
            else avg_time
        )

        if avg_time > 10:  # è¶…é10ç§’
            recommendations.append(
                f"å¹³å‡è™•ç†æ™‚é–“ {avg_time:.1f}s åé«˜ï¼Œå»ºè­°å„ªåŒ– API å‘¼å«æˆ–å¢åŠ å¿«å–"
            )

        if p95_time > 20:  # P95è¶…é20ç§’
            recommendations.append(
                f"P95 è™•ç†æ™‚é–“ {p95_time:.1f}s éé•·ï¼Œå»ºè­°æª¢æŸ¥ç•°å¸¸è«‹æ±‚ä¸¦è¨­å®šè¶…æ™‚"
            )

        # åˆ†æéŒ¯èª¤ç‡
        error_rate = sum(1 for m in metrics if not m.success) / len(metrics)
        if error_rate > 0.05:  # éŒ¯èª¤ç‡è¶…é5%
            recommendations.append(
                f"éŒ¯èª¤ç‡ {error_rate:.1%} éé«˜ï¼Œå»ºè­°æª¢æŸ¥ API é…é¡å’Œç¶²è·¯ç©©å®šæ€§"
            )

        # åˆ†æå¿«å–æ•ˆæœ
        cache_hit_rate = sum(1 for m in metrics if m.cache_hit) / len(metrics)
        if cache_hit_rate < 0.3:  # å¿«å–å‘½ä¸­ç‡ä½æ–¼30%
            recommendations.append(
                f"å¿«å–å‘½ä¸­ç‡ {cache_hit_rate:.1%} åä½ï¼Œå»ºè­°èª¿æ•´å¿«å–ç­–ç•¥æˆ–å¢åŠ å¿«å–å®¹é‡"
            )

        # åˆ†æé‡è©¦æƒ…æ³
        avg_retries = statistics.mean(m.retry_count for m in metrics)
        if avg_retries > 0.5:
            recommendations.append(
                f"å¹³å‡é‡è©¦æ¬¡æ•¸ {avg_retries:.1f} åé«˜ï¼Œå»ºè­°æª¢æŸ¥ API ç©©å®šæ€§"
            )

        return recommendations

    async def _persist_metrics(self, metrics: ProcessingMetrics):
        """æŒä¹…åŒ–æŒ‡æ¨™è³‡æ–™"""
        try:
            # ç°¡å–®çš„æ—¥èªŒè¨˜éŒ„
            log_entry = {
                "timestamp": datetime.fromtimestamp(metrics.start_time).isoformat(),
                "request_id": metrics.request_id,
                "processing_time_ms": metrics.processing_time_ms,
                "success": metrics.success,
                "result_quality": metrics.result_quality,
                "cache_hit": metrics.cache_hit,
                "image_size_kb": round(metrics.image_size_bytes / 1024, 2),
            }

            # å¯«å…¥æ—¥èªŒæª”æ¡ˆ
            log_file = f"performance_log_{datetime.now().strftime('%Y%m%d')}.jsonl"
            async with aiofiles.open(log_file, "a", encoding="utf-8") as f:
                await f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

        except Exception as e:
            print(f"âš ï¸ æ•ˆèƒ½æŒ‡æ¨™æŒä¹…åŒ–å¤±æ•—: {e}")

    def _start_background_monitoring(self):
        """å•Ÿå‹•èƒŒæ™¯ç›£æ§ä»»å‹™"""

        async def health_check_task():
            while True:
                try:
                    await asyncio.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
                    await self._collect_system_health()
                except Exception as e:
                    print(f"âš ï¸ å¥åº·æª¢æŸ¥ä»»å‹™éŒ¯èª¤: {e}")

        # ä¸ç›´æ¥å•Ÿå‹•ï¼Œè®“ä¸»æ‡‰ç”¨æ±ºå®š
        self._background_task = health_check_task

    async def start_monitoring(self):
        """å•Ÿå‹•èƒŒæ™¯ç›£æ§"""
        return asyncio.create_task(self._background_task())

    async def _collect_system_health(self):
        """æ”¶é›†ç³»çµ±å¥åº·è³‡æ–™"""
        try:
            import psutil

            health = SystemHealth(
                timestamp=datetime.now(),
                cpu_usage=psutil.cpu_percent(interval=1),
                memory_usage=psutil.virtual_memory().percent,
                active_connections=len(psutil.net_connections()),
                queue_size=0,  # éœ€è¦å¾å¯¦éš›ä½‡åˆ—ç²å–
                avg_response_time=self._get_recent_avg_response_time(),
                error_rate=self._calculate_recent_error_rate(),
                throughput=self._calculate_current_throughput(),
            )

            self.health_history.append(health)

        except ImportError:
            # psutil æœªå®‰è£ï¼Œè·³éç³»çµ±å¥åº·æª¢æŸ¥
            pass
        except Exception as e:
            print(f"âš ï¸ ç³»çµ±å¥åº·æª¢æŸ¥éŒ¯èª¤: {e}")

    def _get_recent_avg_response_time(self, minutes: int = 5) -> float:
        """ç²å–æœ€è¿‘å¹³å‡å›æ‡‰æ™‚é–“"""
        cutoff = time.time() - (minutes * 60)
        recent = [m for m in self.processing_history if m.start_time > cutoff]
        return statistics.mean(m.processing_time for m in recent) if recent else 0.0

    def _calculate_current_throughput(self, minutes: int = 5) -> float:
        """è¨ˆç®—ç•¶å‰ååé‡"""
        cutoff = time.time() - (minutes * 60)
        recent_count = sum(1 for m in self.processing_history if m.start_time > cutoff)
        return recent_count / minutes  # æ¯åˆ†é˜è«‹æ±‚æ•¸
