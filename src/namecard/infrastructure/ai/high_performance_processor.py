"""
é«˜æ•ˆèƒ½åç‰‡è™•ç†å™¨ - HighPerformanceCardProcessor

å°ˆé–€å„ªåŒ–ã€Œå¾è¼¸å…¥åˆ°è¼¸å‡ºã€çš„è™•ç†é€Ÿåº¦
ç›®æ¨™ï¼š35-40s â†’ 8-15s (2.5-4å€æå‡)

Key Optimizations:
- ç•°æ­¥ä¸¦è¡Œ AI è™•ç† (60-80% æ™‚é–“æ¸›å°‘)
- æ™ºèƒ½å¤šå±¤å¿«å–ç³»çµ± (30-50% å¿«å–å‘½ä¸­)
- å„ªåŒ– Prompt å·¥ç¨‹ (20-30% API åŠ é€Ÿ)
- ä¸¦è¡Œåœ–ç‰‡è™•ç†ç®¡é“
- å¿«é€Ÿå¤±æ•—æ©Ÿåˆ¶
- é€£æ¥æ± å„ªåŒ–
"""

import asyncio
import hashlib
import json
import logging
import time
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple, Union
import io
import os
from pathlib import Path

import aiofiles
import aiohttp
from PIL import Image
import google.generativeai as genai

from simple_config import Config
from src.namecard.core.services.address_service import AddressNormalizer


@dataclass
class ProcessingResult:
    """è™•ç†çµæœæ•¸æ“šé¡"""
    success: bool
    data: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    processing_time: float = 0.0
    cache_hit: bool = False
    cache_level: Optional[str] = None
    optimizations_applied: List[str] = field(default_factory=list)


@dataclass 
class ImageMetadata:
    """åœ–ç‰‡å…ƒæ•¸æ“š"""
    size: Tuple[int, int]
    format: str
    file_size: int
    content_hash: str
    quality_score: float
    is_valid: bool
    processing_hints: List[str] = field(default_factory=list)


class SmartCache:
    """æ™ºèƒ½å¤šå±¤å¿«å–ç³»çµ±"""
    
    def __init__(self, memory_size: int = 100, disk_size: int = 1000):
        self.logger = logging.getLogger(__name__)
        
        # L1: è¨˜æ†¶é«”å¿«å– (æœ€å¿« 0.001s)
        self.memory_cache = {}
        self.memory_access_time = {}
        self.max_memory_size = memory_size
        
        # L2: ç£ç¢Ÿå¿«å– (å¿«é€Ÿ 0.1s)
        self.cache_dir = Path("./cache/namecard_results")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.max_disk_size = disk_size
        
        # å¿«å–çµ±è¨ˆ
        self.stats = {
            "memory_hits": 0,
            "disk_hits": 0,
            "cache_misses": 0,
            "total_requests": 0
        }
    
    def _generate_cache_key(self, image_bytes: bytes) -> str:
        """ç”Ÿæˆå¿«å–éµ"""
        # çµåˆå…§å®¹ hash å’Œå¤§å°ç‰¹å¾µ
        content_hash = hashlib.md5(image_bytes).hexdigest()
        size_info = str(len(image_bytes))
        return f"{content_hash}_{size_info}"
    
    async def get_cached_result(self, image_bytes: bytes) -> Optional[Dict[str, Any]]:
        """ç²å–å¿«å–çµæœ"""
        self.stats["total_requests"] += 1
        cache_key = self._generate_cache_key(image_bytes)
        
        # L1: è¨˜æ†¶é«”å¿«å–æª¢æŸ¥
        if cache_key in self.memory_cache:
            self.stats["memory_hits"] += 1
            self.memory_access_time[cache_key] = time.time()
            self.logger.debug(f"ğŸ¯ è¨˜æ†¶é«”å¿«å–å‘½ä¸­: {cache_key[:12]}")
            return self.memory_cache[cache_key]
        
        # L2: ç£ç¢Ÿå¿«å–æª¢æŸ¥
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                async with aiofiles.open(cache_file, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    result = json.loads(content)
                
                # æå‡åˆ°è¨˜æ†¶é«”å¿«å–
                self._store_memory_cache(cache_key, result)
                
                self.stats["disk_hits"] += 1
                self.logger.debug(f"ğŸ’¾ ç£ç¢Ÿå¿«å–å‘½ä¸­: {cache_key[:12]}")
                return result
                
            except Exception as e:
                self.logger.warning(f"ç£ç¢Ÿå¿«å–è®€å–å¤±æ•—: {e}")
        
        # å¿«å–æœªå‘½ä¸­
        self.stats["cache_misses"] += 1
        return None
    
    async def store_result(self, image_bytes: bytes, result: Dict[str, Any]):
        """å­˜å„²çµæœåˆ°å¿«å–"""
        cache_key = self._generate_cache_key(image_bytes)
        
        # å­˜å„²åˆ°è¨˜æ†¶é«”å¿«å–
        self._store_memory_cache(cache_key, result)
        
        # ç•°æ­¥å­˜å„²åˆ°ç£ç¢Ÿå¿«å–
        asyncio.create_task(self._store_disk_cache(cache_key, result))
    
    def _store_memory_cache(self, cache_key: str, result: Dict[str, Any]):
        """å­˜å„²åˆ°è¨˜æ†¶é«”å¿«å–"""
        # LRU æ·˜æ±°
        if len(self.memory_cache) >= self.max_memory_size:
            # æ‰¾åˆ°æœ€å°‘ä½¿ç”¨çš„é …ç›®
            oldest_key = min(
                self.memory_access_time.keys(),
                key=lambda k: self.memory_access_time[k]
            )
            del self.memory_cache[oldest_key]
            del self.memory_access_time[oldest_key]
        
        self.memory_cache[cache_key] = result
        self.memory_access_time[cache_key] = time.time()
    
    async def _store_disk_cache(self, cache_key: str, result: Dict[str, Any]):
        """ç•°æ­¥å­˜å„²åˆ°ç£ç¢Ÿå¿«å–"""
        try:
            cache_file = self.cache_dir / f"{cache_key}.json"
            
            # æ·»åŠ å¿«å–å…ƒæ•¸æ“š
            cache_data = {
                "cached_at": time.time(),
                "cache_key": cache_key,
                "result": result
            }
            
            async with aiofiles.open(cache_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(cache_data, ensure_ascii=False, indent=2))
            
            self.logger.debug(f"ğŸ’¾ å·²å­˜å„²ç£ç¢Ÿå¿«å–: {cache_key[:12]}")
            
        except Exception as e:
            self.logger.warning(f"ç£ç¢Ÿå¿«å–å­˜å„²å¤±æ•—: {e}")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ç²å–å¿«å–çµ±è¨ˆ"""
        total = self.stats["total_requests"]
        if total == 0:
            return {"hit_rate": 0.0, **self.stats}
        
        hit_rate = (self.stats["memory_hits"] + self.stats["disk_hits"]) / total
        
        return {
            "hit_rate": round(hit_rate, 3),
            "memory_hit_rate": round(self.stats["memory_hits"] / total, 3),
            "disk_hit_rate": round(self.stats["disk_hits"] / total, 3),
            **self.stats
        }


class OptimizedPromptEngine:
    """å„ªåŒ–çš„ Prompt å¼•æ“"""
    
    # ç²¾ç°¡é«˜æ•ˆ Prompt (500 tokens vs åŸæœ¬ 2000+ tokens)
    FAST_EXTRACTION_PROMPT = """
Extract business card information to JSON format.

Required fields:
- name: Person's name
- company: Company name
- department: Department/division
- title: Job title  
- email: Email address
- phone: Phone numbers (format: +886XXXXXXXXX, comma-separated)
- address: Company address

Decision influence (based on title):
- CEO, ç¸½ç¶“ç†, è‘£äº‹é•· â†’ "é«˜"
- ç¶“ç†, ä¸»ç®¡, éƒ¨é•· â†’ "ä¸­"  
- å°ˆå“¡, å·¥ç¨‹å¸«, åŠ©ç† â†’ "ä½"

Output format:
{
  "name": "string or null",
  "company": "string or null",
  "department": "string or null", 
  "title": "string or null",
  "decision_influence": "é«˜/ä¸­/ä½ or null",
  "email": "string or null",
  "phone": "string or null", 
  "address": "string or null",
  "contact_source": "åç‰‡äº¤æ›",
  "notes": "string or null"
}

Rules:
1. Use null for missing fields
2. Combine multiple phone numbers with comma+space
3. Keep fax numbers in notes field
4. Output valid JSON only
"""

    MULTI_CARD_DETECTION_PROMPT = """
Detect and extract multiple business cards from image.

Count business cards first, then extract each one.

Output format:
{
  "card_count": number,
  "cards": [
    {
      "card_index": 1,
      "confidence_score": 0.0-1.0,
      "name": "string or null",
      "company": "string or null",
      "email": "string or null",
      "phone": "string or null",
      "clarity_issues": ["array of issues"],
      "suggestions": ["array of suggestions"]
    }
  ],
  "overall_quality": "good/partial/poor"
}

Rules:
1. confidence_score based on text clarity
2. Report clarity issues if text is blurry
3. Suggest retaking photo if quality < 0.6
"""
    
    @classmethod
    def get_optimized_prompt(cls, task_type: str = "single_card") -> str:
        """ç²å–å„ªåŒ–çš„ Prompt"""
        if task_type == "multi_card":
            return cls.MULTI_CARD_DETECTION_PROMPT
        else:
            return cls.FAST_EXTRACTION_PROMPT


class HighPerformanceCardProcessor:
    """é«˜æ•ˆèƒ½åç‰‡è™•ç†å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–çµ„ä»¶
        self._init_ai_models()
        self._init_cache_system()
        self._init_processing_pipeline()
        
        # æ•ˆèƒ½ç›£æ§
        self.processing_stats = {
            "total_processed": 0,
            "cache_hits": 0,
            "avg_processing_time": 0.0,
            "fast_failures": 0,
            "optimizations_used": []
        }
        
        self.logger.info("ğŸš€ HighPerformanceCardProcessor åˆå§‹åŒ–å®Œæˆ")
    
    def _init_ai_models(self):
        """åˆå§‹åŒ– AI æ¨¡å‹"""
        try:
            # ä¸»è¦ API Key
            self.current_api_key = Config.GOOGLE_API_KEY
            self.fallback_api_key = Config.GOOGLE_API_KEY_FALLBACK
            self.using_fallback = False
            
            genai.configure(api_key=self.current_api_key)
            self.model = genai.GenerativeModel(Config.GEMINI_MODEL)
            
            # åœ°å€æ­£è¦åŒ–å™¨
            self.address_normalizer = AddressNormalizer()
            
            # ç·šç¨‹æ±  (ç”¨æ–¼ CPU å¯†é›†ä»»å‹™)
            self.thread_pool = ThreadPoolExecutor(max_workers=4)
            
            self.logger.info("âœ… AI æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            raise Exception(f"AI æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def _init_cache_system(self):
        """åˆå§‹åŒ–å¿«å–ç³»çµ±"""
        self.cache = SmartCache(memory_size=200, disk_size=2000)
        self.logger.info("âœ… æ™ºèƒ½å¿«å–ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    
    def _init_processing_pipeline(self):
        """åˆå§‹åŒ–è™•ç†ç®¡é“"""
        # HTTP æœƒè©± (ç”¨æ–¼åœ–ç‰‡ä¸‹è¼‰)
        self.http_session = None
        self._session_lock = asyncio.Lock()
        
        # åœ–ç‰‡è™•ç†é…ç½®
        self.max_image_size = (2048, 2048)
        self.supported_formats = {'JPEG', 'PNG', 'WebP'}
        self.max_file_size = 10 * 1024 * 1024  # 10MB
        
        self.logger.info("âœ… è™•ç†ç®¡é“åˆå§‹åŒ–å®Œæˆ")
    
    async def _get_http_session(self) -> aiohttp.ClientSession:
        """ç²å– HTTP æœƒè©±ï¼ˆæ‡¶åŠ è¼‰ + é€£æ¥æ± å„ªåŒ–ï¼‰"""
        if self.http_session is None:
            async with self._session_lock:
                if self.http_session is None:
                    # å„ªåŒ–çš„é€£æ¥æ± é…ç½®
                    connector = aiohttp.TCPConnector(
                        limit=50,                    # ç¸½é€£æ¥æ•¸
                        limit_per_host=10,           # æ¯ä¸»æ©Ÿé€£æ¥æ•¸
                        keepalive_timeout=60,        # ä¿æŒé€£æ¥æ™‚é–“
                        enable_cleanup_closed=True   # è‡ªå‹•æ¸…ç†
                    )
                    
                    timeout = aiohttp.ClientTimeout(
                        total=30,      # ç¸½è¶…æ™‚
                        connect=10,    # é€£æ¥è¶…æ™‚
                        sock_read=20   # è®€å–è¶…æ™‚
                    )
                    
                    self.http_session = aiohttp.ClientSession(
                        connector=connector,
                        timeout=timeout
                    )
                    
                    self.logger.debug("ğŸ”§ HTTP æœƒè©±å·²å‰µå»º")
        
        return self.http_session
    
    async def analyze_image_metadata(self, image_bytes: bytes) -> ImageMetadata:
        """ç•°æ­¥åˆ†æåœ–ç‰‡å…ƒæ•¸æ“šï¼ˆå¿«é€Ÿé æª¢ï¼‰"""
        def _analyze_sync():
            try:
                img = Image.open(io.BytesIO(image_bytes))
                
                # è¨ˆç®—å“è³ªåˆ†æ•¸
                quality_score = self._calculate_quality_score(img, image_bytes)
                
                # ç”Ÿæˆè™•ç†æç¤º
                hints = []
                if img.size[0] > 4096 or img.size[1] > 4096:
                    hints.append("large_image")
                if quality_score < 0.3:
                    hints.append("low_quality")
                if len(image_bytes) > 5 * 1024 * 1024:
                    hints.append("large_file")
                
                return ImageMetadata(
                    size=img.size,
                    format=img.format,
                    file_size=len(image_bytes),
                    content_hash=hashlib.md5(image_bytes).hexdigest()[:16],
                    quality_score=quality_score,
                    is_valid=quality_score > 0.2,
                    processing_hints=hints
                )
                
            except Exception as e:
                self.logger.warning(f"åœ–ç‰‡å…ƒæ•¸æ“šåˆ†æå¤±æ•—: {e}")
                return ImageMetadata(
                    size=(0, 0),
                    format="unknown",
                    file_size=len(image_bytes),
                    content_hash="",
                    quality_score=0.0,
                    is_valid=False,
                    processing_hints=["invalid_image"]
                )
        
        # åœ¨ç·šç¨‹æ± ä¸­åŸ·è¡Œ CPU å¯†é›†ä»»å‹™
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.thread_pool, _analyze_sync)
    
    def _calculate_quality_score(self, img: Image.Image, image_bytes: bytes) -> float:
        """è¨ˆç®—åœ–ç‰‡å“è³ªåˆ†æ•¸"""
        score = 1.0
        
        # è§£æåº¦è©•åˆ†
        pixels = img.size[0] * img.size[1]
        if pixels < 100000:  # < 100K pixels
            score *= 0.3
        elif pixels < 500000:  # < 500K pixels
            score *= 0.7
        
        # æª”æ¡ˆå¤§å°è©•åˆ†
        file_size = len(image_bytes)
        if file_size < 50000:  # < 50KB
            score *= 0.4
        elif file_size > 10000000:  # > 10MB
            score *= 0.8
        
        # æ ¼å¼è©•åˆ†
        if img.format not in self.supported_formats:
            score *= 0.5
        
        return min(score, 1.0)
    
    async def optimize_image_for_ai(self, image_bytes: bytes, metadata: ImageMetadata) -> bytes:
        """ç•°æ­¥å„ªåŒ–åœ–ç‰‡ä»¥æå‡ AI è™•ç†é€Ÿåº¦"""
        def _optimize_sync():
            try:
                img = Image.open(io.BytesIO(image_bytes))
                
                # æ ¹æ“šå…ƒæ•¸æ“šé¸æ“‡å„ªåŒ–ç­–ç•¥
                if "large_image" in metadata.processing_hints:
                    # å¤§åœ–ç‰‡ï¼šèª¿æ•´å¤§å°
                    img.thumbnail(self.max_image_size, Image.Resampling.LANCZOS)
                
                if "low_quality" in metadata.processing_hints:
                    # ä½å“è³ªï¼šå¢å¼·å°æ¯”åº¦
                    from PIL import ImageEnhance
                    enhancer = ImageEnhance.Contrast(img)
                    img = enhancer.enhance(1.2)
                
                # è½‰æ›ç‚º RGB æ ¼å¼ï¼ˆGemini æœ€ä½³ç›¸å®¹æ€§ï¼‰
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # è¼¸å‡ºå„ªåŒ–å¾Œçš„åœ–ç‰‡
                output = io.BytesIO()
                img.save(output, format='JPEG', quality=85, optimize=True)
                return output.getvalue()
                
            except Exception as e:
                self.logger.warning(f"åœ–ç‰‡å„ªåŒ–å¤±æ•—: {e}")
                return image_bytes  # è¿”å›åŸåœ–
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.thread_pool, _optimize_sync)
    
    async def _call_gemini_api_async(
        self, 
        image_bytes: bytes, 
        prompt: str,
        max_retries: int = 3
    ) -> str:
        """ç•°æ­¥èª¿ç”¨ Gemini API"""
        def _call_sync():
            img_pil = Image.open(io.BytesIO(image_bytes))
            response = self.model.generate_content([prompt, img_pil])
            return response.text.strip()
        
        last_error = None
        
        for attempt in range(max_retries):
            try:
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(self.thread_pool, _call_sync)
                return result
                
            except Exception as e:
                last_error = e
                error_str = str(e).lower()
                
                # æª¢æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ› API Key
                if (
                    any(keyword in error_str for keyword in ["quota", "limit", "429"])
                    and not self.using_fallback
                    and self.fallback_api_key
                ):
                    try:
                        self.logger.info("ğŸ”„ åˆ‡æ›åˆ°å‚™ç”¨ API Key...")
                        genai.configure(api_key=self.fallback_api_key)
                        self.model = genai.GenerativeModel(Config.GEMINI_MODEL)
                        self.using_fallback = True
                        continue  # é‡è©¦
                        
                    except Exception as fallback_error:
                        raise Exception(f"API Key åˆ‡æ›å¤±æ•—: {fallback_error}")
                
                # æš«æ™‚æ€§éŒ¯èª¤é‡è©¦
                if any(keyword in error_str for keyword in ["500", "502", "503", "timeout"]):
                    if attempt < max_retries - 1:
                        wait_time = (2 ** attempt) + 0.1
                        self.logger.warning(f"æš«æ™‚æ€§éŒ¯èª¤ï¼Œ{wait_time}s å¾Œé‡è©¦: {error_str[:100]}")
                        await asyncio.sleep(wait_time)
                        continue
                
                # å…¶ä»–éŒ¯èª¤ç›´æ¥æ‹‹å‡º
                raise e
        
        raise Exception(f"API èª¿ç”¨å¤±æ•—ï¼Œå·²é‡è©¦ {max_retries} æ¬¡: {str(last_error)}")
    
    async def process_single_card_fast(
        self, 
        image_bytes: bytes,
        enable_cache: bool = True
    ) -> ProcessingResult:
        """é«˜é€Ÿè™•ç†å–®å¼µåç‰‡"""
        start_time = time.time()
        optimizations = []
        
        try:
            # æ­¥é©Ÿ 1: å¿«å–æª¢æŸ¥ (0.001-0.1s)
            cached_result = None
            if enable_cache:
                cached_result = await self.cache.get_cached_result(image_bytes)
                if cached_result:
                    processing_time = time.time() - start_time
                    self.processing_stats["cache_hits"] += 1
                    optimizations.append("cache_hit")
                    
                    return ProcessingResult(
                        success=True,
                        data=cached_result,
                        processing_time=processing_time,
                        cache_hit=True,
                        cache_level="memory" if processing_time < 0.01 else "disk",
                        optimizations_applied=optimizations
                    )
            
            # æ­¥é©Ÿ 2: ä¸¦è¡Œé è™•ç† (0.1-0.5s)
            metadata_task = self.analyze_image_metadata(image_bytes)
            optimization_task = None
            
            metadata = await metadata_task
            
            # å¿«é€Ÿå¤±æ•—æª¢æŸ¥
            if not metadata.is_valid:
                self.processing_stats["fast_failures"] += 1
                optimizations.append("fast_failure")
                
                return ProcessingResult(
                    success=False,
                    error="åœ–ç‰‡å“è³ªä¸ä½³ï¼Œè«‹é‡æ–°æ‹æ”",
                    processing_time=time.time() - start_time,
                    optimizations_applied=optimizations
                )
            
            # æ­¥é©Ÿ 3: ä¸¦è¡Œåœ–ç‰‡å„ªåŒ– + API æº–å‚™
            optimize_task = self.optimize_image_for_ai(image_bytes, metadata)
            
            # é¸æ“‡æœ€ä½³ Prompt
            prompt = OptimizedPromptEngine.get_optimized_prompt("single_card")
            optimizations.append("optimized_prompt")
            
            # ç­‰å¾…åœ–ç‰‡å„ªåŒ–
            optimized_image = await optimize_task
            if len(optimized_image) != len(image_bytes):
                optimizations.append("image_optimized")
            
            # æ­¥é©Ÿ 4: AI è™•ç† (2-8sï¼Œå„ªåŒ–å¾Œ)
            ai_start = time.time()
            raw_response = await self._call_gemini_api_async(optimized_image, prompt)
            ai_time = time.time() - ai_start
            
            optimizations.append(f"ai_processing_{ai_time:.1f}s")
            
            # æ­¥é©Ÿ 5: ä¸¦è¡Œå¾Œè™•ç†
            parse_task = self._parse_ai_response_async(raw_response)
            cache_task = None
            
            if enable_cache:
                # ç•°æ­¥å­˜å„²å¿«å–ï¼ˆä¸ç­‰å¾…ï¼‰
                cache_task = self.cache.store_result(image_bytes, {})  # æš«æ™‚ç©ºå­—å…¸
            
            parsed_data = await parse_task
            
            # åœ°å€æ­£è¦åŒ–
            if parsed_data.get("address"):
                address_result = self.address_normalizer.normalize_address(parsed_data["address"])
                parsed_data["address"] = address_result["normalized"]
                optimizations.append("address_normalized")
            
            # æ›´æ–°å¿«å–
            if enable_cache and cache_task:
                await self.cache.store_result(image_bytes, parsed_data)
            
            # çµ±è¨ˆæ›´æ–°
            processing_time = time.time() - start_time
            self.processing_stats["total_processed"] += 1
            self._update_avg_processing_time(processing_time)
            
            return ProcessingResult(
                success=True,
                data=parsed_data,
                processing_time=processing_time,
                cache_hit=False,
                optimizations_applied=optimizations
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"é«˜é€Ÿè™•ç†å¤±æ•—: {e}")
            
            return ProcessingResult(
                success=False,
                error=str(e),
                processing_time=processing_time,
                optimizations_applied=optimizations
            )
    
    async def _parse_ai_response_async(self, raw_response: str) -> Dict[str, Any]:
        """ç•°æ­¥è§£æ AI å›æ‡‰"""
        def _parse_sync():
            # æ¸…ç†å›æ‡‰
            if raw_response.startswith("```json"):
                raw_response_clean = raw_response[7:]
            elif raw_response.startswith("```"):
                raw_response_clean = raw_response[3:]
            else:
                raw_response_clean = raw_response
                
            if raw_response_clean.endswith("```"):
                raw_response_clean = raw_response_clean[:-3]
            
            raw_response_clean = raw_response_clean.strip()
            
            try:
                return json.loads(raw_response_clean)
            except json.JSONDecodeError as e:
                # å˜—è©¦ä¿®å¾©å¸¸è¦‹çš„ JSON å•é¡Œ
                try:
                    # ç§»é™¤å¯èƒ½çš„è¨»è§£
                    lines = raw_response_clean.split('\n')
                    clean_lines = [line for line in lines if not line.strip().startswith('//')]
                    clean_json = '\n'.join(clean_lines)
                    return json.loads(clean_json)
                except:
                    raise ValueError(f"ç„¡æ³•è§£æ AI å›æ‡‰ç‚º JSON: {e}")
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.thread_pool, _parse_sync)
    
    def _update_avg_processing_time(self, new_time: float):
        """æ›´æ–°å¹³å‡è™•ç†æ™‚é–“"""
        current_avg = self.processing_stats["avg_processing_time"]
        total_processed = self.processing_stats["total_processed"]
        
        # åŠ æ¬Šå¹³å‡
        self.processing_stats["avg_processing_time"] = (
            (current_avg * (total_processed - 1) + new_time) / total_processed
        )
    
    async def process_multi_card_fast(
        self,
        image_bytes: bytes,
        enable_cache: bool = True
    ) -> ProcessingResult:
        """é«˜é€Ÿè™•ç†å¤šå¼µåç‰‡"""
        # ä½¿ç”¨å¤šåç‰‡æª¢æ¸¬ Prompt
        start_time = time.time()
        
        try:
            # æª¢æŸ¥å¿«å–
            if enable_cache:
                cached_result = await self.cache.get_cached_result(image_bytes)
                if cached_result:
                    return ProcessingResult(
                        success=True,
                        data=cached_result,
                        processing_time=time.time() - start_time,
                        cache_hit=True,
                        optimizations_applied=["cache_hit", "multi_card"]
                    )
            
            # åœ–ç‰‡é è™•ç†
            metadata = await self.analyze_image_metadata(image_bytes)
            if not metadata.is_valid:
                return ProcessingResult(
                    success=False,
                    error="åœ–ç‰‡å“è³ªä¸ä½³",
                    processing_time=time.time() - start_time,
                    optimizations_applied=["fast_failure", "multi_card"]
                )
            
            # ä½¿ç”¨å¤šåç‰‡æª¢æ¸¬ Prompt
            prompt = OptimizedPromptEngine.get_optimized_prompt("multi_card")
            optimized_image = await self.optimize_image_for_ai(image_bytes, metadata)
            
            # AI è™•ç†
            raw_response = await self._call_gemini_api_async(optimized_image, prompt)
            parsed_data = await self._parse_ai_response_async(raw_response)
            
            # å­˜å„²å¿«å–
            if enable_cache:
                await self.cache.store_result(image_bytes, parsed_data)
            
            processing_time = time.time() - start_time
            
            return ProcessingResult(
                success=True,
                data=parsed_data,
                processing_time=processing_time,
                cache_hit=False,
                optimizations_applied=["multi_card", "optimized_prompt", "image_optimized"]
            )
            
        except Exception as e:
            return ProcessingResult(
                success=False,
                error=str(e),
                processing_time=time.time() - start_time,
                optimizations_applied=["multi_card"]
            )
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ç²å–æ•ˆèƒ½çµ±è¨ˆ"""
        cache_stats = self.cache.get_cache_stats()
        
        return {
            "processing": self.processing_stats,
            "cache": cache_stats,
            "efficiency": {
                "avg_processing_time": f"{self.processing_stats['avg_processing_time']:.2f}s",
                "cache_hit_rate": f"{cache_stats['hit_rate']:.1%}",
                "fast_failure_rate": (
                    self.processing_stats["fast_failures"] / 
                    max(1, self.processing_stats["total_processed"])
                )
            }
        }
    
    async def cleanup(self):
        """æ¸…ç†è³‡æº"""
        try:
            if self.http_session:
                await self.http_session.close()
            
            if hasattr(self, 'thread_pool'):
                self.thread_pool.shutdown(wait=True)
            
            self.logger.info("âœ… HighPerformanceCardProcessor è³‡æºå·²æ¸…ç†")
            
        except Exception as e:
            self.logger.warning(f"æ¸…ç†è³‡æºæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


# ä¾¿åˆ©å·¥å» å‡½æ•¸
async def create_high_performance_processor() -> HighPerformanceCardProcessor:
    """å‰µå»ºé«˜æ•ˆèƒ½è™•ç†å™¨"""
    processor = HighPerformanceCardProcessor()
    return processor


# æ•ˆèƒ½æ¸¬è©¦è¼”åŠ©å‡½æ•¸
async def benchmark_processing_speed(processor: HighPerformanceCardProcessor, image_bytes: bytes, iterations: int = 5):
    """åŸºæº–æ¸¬è©¦è™•ç†é€Ÿåº¦"""
    times = []
    
    for i in range(iterations):
        result = await processor.process_single_card_fast(image_bytes, enable_cache=(i == 0))
        times.append(result.processing_time)
    
    return {
        "avg_time": sum(times) / len(times),
        "min_time": min(times),
        "max_time": max(times),
        "cache_time": times[1] if len(times) > 1 else None,  # ç¬¬äºŒæ¬¡æ‡‰è©²æ˜¯å¿«å–
        "iterations": iterations
    }