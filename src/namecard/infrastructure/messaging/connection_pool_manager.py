#!/usr/bin/env python3
"""
é€£æ¥æ± ç®¡ç†å™¨ - ConnectionPoolManager
å°ˆé–€è§£æ±ºä¸¦è¡Œä¸‹è¼‰å™¨çš„é€£æ¥æ± å•é¡Œ

æ ¸å¿ƒä¿®å¾©ï¼š
1. é€£æ¥æ± æ´©æ¼å•é¡Œ
2. è¶…æ™‚å’Œé‡è©¦æ©Ÿåˆ¶
3. é€£æ¥å¾©ç”¨å„ªåŒ–
4. è³‡æºæ¸…ç†è‡ªå‹•åŒ–
5. ä¸¦ç™¼æ§åˆ¶å„ªåŒ–
"""

import asyncio
import logging
import time
import weakref
from contextlib import asynccontextmanager
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Set, Union
from urllib.parse import urlparse

import aiohttp


@dataclass
class ConnectionPoolConfig:
    """é€£æ¥æ± é…ç½®"""

    # åŸºæœ¬é€£æ¥é™åˆ¶
    total_limit: int = 25
    per_host_limit: int = 8

    # è¶…æ™‚è¨­ç½®
    total_timeout: int = 20
    connect_timeout: int = 5
    read_timeout: int = 15

    # Keep-alive è¨­ç½®
    keepalive_timeout: int = 30
    keepalive_expiry: int = 30

    # æ¸…ç†å’Œç¶­è­·
    enable_cleanup_closed: bool = True
    cleanup_interval: int = 60

    # é‡è©¦è¨­ç½®
    max_retries: int = 3
    retry_backoff: float = 1.0

    # DNS å„ªåŒ–
    use_dns_cache: bool = True
    dns_cache_ttl: int = 300


@dataclass
class ConnectionStats:
    """é€£æ¥çµ±è¨ˆ"""

    active_connections: int = 0
    idle_connections: int = 0
    total_created: int = 0
    total_closed: int = 0
    total_failed: int = 0
    total_timeouts: int = 0
    cache_hits: int = 0
    cache_misses: int = 0


class ConnectionPoolManager:
    """é€£æ¥æ± ç®¡ç†å™¨ - è§£æ±ºé€£æ¥æ± ç›¸é—œå•é¡Œ"""

    def __init__(self, config: Optional[ConnectionPoolConfig] = None):
        self.config = config or ConnectionPoolConfig()
        self.logger = logging.getLogger(__name__)

        # é€£æ¥æ± å¯¦ä¾‹ç®¡ç†
        self._sessions: Dict[str, aiohttp.ClientSession] = {}
        self._session_locks: Dict[str, asyncio.Lock] = {}
        self._session_refs = weakref.WeakSet()

        # çµ±è¨ˆå’Œç›£æ§
        self.stats = ConnectionStats()
        self._last_cleanup = time.time()

        # ä¿¡è™Ÿé‡æ§åˆ¶ä¸¦ç™¼
        self._semaphore = asyncio.Semaphore(self.config.total_limit)

        # å•Ÿå‹•æ¸…ç†ä»»å‹™
        self._cleanup_task = None
        self._shutdown = False

        self.logger.info("ğŸ”§ ConnectionPoolManager åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"   - ç¸½é€£æ¥é™åˆ¶: {self.config.total_limit}")
        self.logger.info(f"   - æ¯ä¸»æ©Ÿé™åˆ¶: {self.config.per_host_limit}")
        self.logger.info(f"   - ç¸½è¶…æ™‚: {self.config.total_timeout}s")

    async def get_session(self, session_key: str = "default") -> aiohttp.ClientSession:
        """ç²å–å„ªåŒ–çš„ HTTP æœƒè©±ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰"""
        if session_key not in self._sessions:
            if session_key not in self._session_locks:
                self._session_locks[session_key] = asyncio.Lock()

            async with self._session_locks[session_key]:
                if session_key not in self._sessions:
                    session = await self._create_optimized_session()
                    self._sessions[session_key] = session
                    self._session_refs.add(session)

                    self.stats.total_created += 1
                    self.logger.debug(f"âœ… æ–°å»º HTTP Session: {session_key}")

        return self._sessions[session_key]

    async def _create_optimized_session(self) -> aiohttp.ClientSession:
        """å‰µå»ºå„ªåŒ–çš„ HTTP æœƒè©±"""
        # é€£æ¥å™¨é…ç½® - ä¿®å¾©é€£æ¥æ± å•é¡Œ
        connector_config = {
            "limit": self.config.total_limit,
            "limit_per_host": self.config.per_host_limit,
            "keepalive_timeout": self.config.keepalive_timeout,
            "enable_cleanup_closed": self.config.enable_cleanup_closed,
            # ğŸ”§ é—œéµä¿®å¾©ï¼šå•Ÿç”¨é€£æ¥å¾©ç”¨å’Œå„ªåŒ–
            "use_dns_cache": self.config.use_dns_cache,
            "ttl_dns_cache": self.config.dns_cache_ttl,
            "ssl": None,  # è®“ aiohttp è‡ªå‹•æ±ºå®š
            # ğŸ”§ ä¿®å¾©ï¼šé€£æ¥æ± ç¶­è­·è¨­ç½®
            "force_close": False,  # é¿å…å¼·åˆ¶é—œé–‰é€£æ¥
        }

        connector = aiohttp.TCPConnector(**connector_config)

        # è¶…æ™‚é…ç½® - ä¿®å¾©è¶…æ™‚å•é¡Œ
        timeout = aiohttp.ClientTimeout(
            total=self.config.total_timeout,
            connect=self.config.connect_timeout,
            sock_read=self.config.read_timeout,
        )

        # è«‹æ±‚é ­å„ªåŒ–
        headers = {
            "User-Agent": "NameCard-Bot/2.0 (Connection-Pool-Optimized)",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Keep-Alive": f"timeout={self.config.keepalive_timeout}",
        }

        session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers=headers,
            auto_decompress=True,
            raise_for_status=False,  # æ‰‹å‹•è™•ç†éŒ¯èª¤
        )

        return session

    @asynccontextmanager
    async def request_context(
        self, url: str, method: str = "GET", session_key: str = "default", **kwargs
    ):
        """è«‹æ±‚ä¸Šä¸‹æ–‡ç®¡ç†å™¨ - ç¢ºä¿è³‡æºæ­£ç¢ºé‡‹æ”¾"""
        session = await self.get_session(session_key)

        async with self._semaphore:  # æ§åˆ¶ä¸¦ç™¼
            start_time = time.time()

            try:
                async with session.request(method, url, **kwargs) as response:
                    self.stats.active_connections += 1
                    yield response

            except asyncio.TimeoutError:
                self.stats.total_timeouts += 1
                self.logger.warning(f"â° è«‹æ±‚è¶…æ™‚: {url}")
                raise

            except aiohttp.ClientError as e:
                self.stats.total_failed += 1
                self.logger.warning(f"âŒ è«‹æ±‚å¤±æ•—: {url} - {e}")
                raise

            finally:
                self.stats.active_connections = max(
                    0, self.stats.active_connections - 1
                )
                duration = time.time() - start_time
                if duration > 5.0:  # è­¦å‘Šæ…¢è«‹æ±‚
                    self.logger.warning(f"ğŸŒ æ…¢è«‹æ±‚: {url} ({duration:.2f}s)")

    async def download_with_retry(
        self, url: str, max_retries: Optional[int] = None, session_key: str = "default"
    ) -> Dict[str, Any]:
        """å¸¶é‡è©¦æ©Ÿåˆ¶çš„ä¸‹è¼‰ - ä¿®å¾©ä¸ç©©å®šé€£æ¥å•é¡Œ"""
        max_retries = max_retries or self.config.max_retries
        last_error = None

        for attempt in range(max_retries + 1):
            try:
                async with self.request_context(
                    url, session_key=session_key
                ) as response:
                    if response.status == 200:
                        data = await response.read()
                        return {
                            "success": True,
                            "data": data,
                            "size": len(data),
                            "attempt": attempt + 1,
                            "url": url,
                        }
                    else:
                        raise aiohttp.ClientResponseError(
                            request_info=response.request_info,
                            history=response.history,
                            status=response.status,
                        )

            except Exception as e:
                last_error = e

                if attempt < max_retries:
                    # æŒ‡æ•¸é€€é¿é‡è©¦
                    delay = self.config.retry_backoff * (2**attempt)
                    self.logger.debug(
                        f"ğŸ”„ é‡è©¦ {attempt + 1}/{max_retries}: {url} "
                        f"(å»¶é² {delay:.1f}s)"
                    )
                    await asyncio.sleep(delay)
                else:
                    self.logger.error(f"âŒ æ‰€æœ‰é‡è©¦å¤±æ•—: {url} - {last_error}")

        return {
            "success": False,
            "error": str(last_error),
            "attempt": max_retries + 1,
            "url": url,
        }

    async def batch_download(
        self, urls: List[str], max_concurrent: int = 8, session_key: str = "batch"
    ) -> List[Dict[str, Any]]:
        """æ‰¹æ¬¡ä¸‹è¼‰ - å„ªåŒ–ä¸¦ç™¼æ§åˆ¶"""
        if not urls:
            return []

        self.logger.info(
            f"ğŸš€ é–‹å§‹æ‰¹æ¬¡ä¸‹è¼‰ {len(urls)} å€‹ URL (ä¸¦ç™¼åº¦: {max_concurrent})"
        )

        # å‰µå»ºä¿¡è™Ÿé‡æ§åˆ¶ä¸¦ç™¼
        semaphore = asyncio.Semaphore(max_concurrent)

        async def download_with_semaphore(url: str) -> Dict[str, Any]:
            async with semaphore:
                return await self.download_with_retry(url, session_key=session_key)

        # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰ä¸‹è¼‰
        start_time = time.time()
        results = await asyncio.gather(
            *[download_with_semaphore(url) for url in urls], return_exceptions=True
        )

        # è™•ç†çµæœ
        processed_results = []
        successful_count = 0

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append(
                    {
                        "success": False,
                        "error": str(result),
                        "url": urls[i] if i < len(urls) else "unknown",
                    }
                )
            else:
                processed_results.append(result)
                if result.get("success"):
                    successful_count += 1

        duration = time.time() - start_time
        success_rate = successful_count / len(urls) if urls else 0

        self.logger.info(
            f"âœ… æ‰¹æ¬¡ä¸‹è¼‰å®Œæˆ: {successful_count}/{len(urls)} "
            f"({success_rate:.1%}) in {duration:.2f}s"
        )

        return processed_results

    async def get_connection_stats(self) -> Dict[str, Any]:
        """ç²å–é€£æ¥æ± çµ±è¨ˆä¿¡æ¯"""
        total_sessions = len(self._sessions)

        # è¨ˆç®—æ¯å€‹ session çš„é€£æ¥çµ±è¨ˆ
        session_stats = {}
        for key, session in self._sessions.items():
            if hasattr(session.connector, "_conns"):
                active = len(
                    [
                        conn
                        for conns in session.connector._conns.values()
                        for conn in conns
                    ]
                )
                session_stats[key] = {
                    "active_connections": active,
                    "closed": session.closed,
                }

        return {
            "total_sessions": total_sessions,
            "session_details": session_stats,
            "global_stats": {
                "active_connections": self.stats.active_connections,
                "total_created": self.stats.total_created,
                "total_closed": self.stats.total_closed,
                "total_failed": self.stats.total_failed,
                "total_timeouts": self.stats.total_timeouts,
            },
            "config": {
                "total_limit": self.config.total_limit,
                "per_host_limit": self.config.per_host_limit,
                "total_timeout": self.config.total_timeout,
            },
        }

    async def cleanup_idle_connections(self):
        """æ¸…ç†é–’ç½®é€£æ¥ - ä¿®å¾©é€£æ¥æ± æ´©æ¼"""
        current_time = time.time()

        if current_time - self._last_cleanup < self.config.cleanup_interval:
            return  # é‚„ä¸éœ€è¦æ¸…ç†

        self.logger.debug("ğŸ§¹ é–‹å§‹æ¸…ç†é–’ç½®é€£æ¥")

        cleanup_count = 0
        for session_key, session in list(self._sessions.items()):
            if session.closed:
                # ç§»é™¤å·²é—œé–‰çš„ session
                del self._sessions[session_key]
                if session_key in self._session_locks:
                    del self._session_locks[session_key]
                cleanup_count += 1
                self.stats.total_closed += 1
                continue

            # æ¸…ç† session çš„é–’ç½®é€£æ¥
            if hasattr(session.connector, "_cleanup_closed_disabled"):
                # å¼·åˆ¶å•Ÿç”¨æ¸…ç†
                session.connector._cleanup_closed_disabled = False

            # èª¿ç”¨é€£æ¥å™¨çš„æ¸…ç†æ–¹æ³•
            if hasattr(session.connector, "_cleanup_closed"):
                try:
                    await session.connector._cleanup_closed()
                except Exception as e:
                    self.logger.warning(f"æ¸…ç†é€£æ¥å¤±æ•—: {e}")

        self._last_cleanup = current_time

        if cleanup_count > 0:
            self.logger.info(f"ğŸ§¹ æ¸…ç†å®Œæˆï¼Œç§»é™¤ {cleanup_count} å€‹é–’ç½® session")

    async def force_cleanup_all(self):
        """å¼·åˆ¶æ¸…ç†æ‰€æœ‰é€£æ¥"""
        self.logger.info("ğŸ§¹ å¼·åˆ¶æ¸…ç†æ‰€æœ‰é€£æ¥æ± ")

        for session_key, session in list(self._sessions.items()):
            try:
                if not session.closed:
                    await session.close()
                    self.stats.total_closed += 1
            except Exception as e:
                self.logger.warning(f"é—œé–‰ session {session_key} å¤±æ•—: {e}")

        self._sessions.clear()
        self._session_locks.clear()
        self.logger.info("âœ… æ‰€æœ‰é€£æ¥æ± å·²æ¸…ç†")

    async def start_background_cleanup(self):
        """å•Ÿå‹•èƒŒæ™¯æ¸…ç†ä»»å‹™"""
        if self._cleanup_task is not None:
            return  # å·²ç¶“å•Ÿå‹•

        async def cleanup_loop():
            while not self._shutdown:
                try:
                    await self.cleanup_idle_connections()
                    await asyncio.sleep(self.config.cleanup_interval)
                except asyncio.CancelledError:
                    break
                except Exception as e:
                    self.logger.error(f"èƒŒæ™¯æ¸…ç†éŒ¯èª¤: {e}")
                    await asyncio.sleep(10)  # éŒ¯èª¤å¾Œç­‰å¾… 10 ç§’

        self._cleanup_task = asyncio.create_task(cleanup_loop())
        self.logger.info("ğŸ”„ èƒŒæ™¯æ¸…ç†ä»»å‹™å·²å•Ÿå‹•")

    async def shutdown(self):
        """é—œé–‰é€£æ¥æ± ç®¡ç†å™¨"""
        self.logger.info("ğŸ›‘ é—œé–‰é€£æ¥æ± ç®¡ç†å™¨")

        self._shutdown = True

        # å–æ¶ˆèƒŒæ™¯ä»»å‹™
        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass

        # å¼·åˆ¶æ¸…ç†æ‰€æœ‰é€£æ¥
        await self.force_cleanup_all()

        self.logger.info("âœ… é€£æ¥æ± ç®¡ç†å™¨å·²é—œé–‰")

    def __del__(self):
        """ææ§‹å‡½æ•¸ - ç¢ºä¿è³‡æºé‡‹æ”¾"""
        if not self._shutdown and self._sessions:
            self.logger.warning("âš ï¸ ConnectionPoolManager æœªæ­£ç¢ºé—œé–‰ï¼Œå­˜åœ¨è³‡æºæ´©æ¼é¢¨éšª")


# å…¨åŸŸé€£æ¥æ± ç®¡ç†å™¨å¯¦ä¾‹
_global_pool_manager: Optional[ConnectionPoolManager] = None


async def get_global_pool_manager() -> ConnectionPoolManager:
    """ç²å–å…¨åŸŸé€£æ¥æ± ç®¡ç†å™¨"""
    global _global_pool_manager

    if _global_pool_manager is None:
        _global_pool_manager = ConnectionPoolManager()
        await _global_pool_manager.start_background_cleanup()

    return _global_pool_manager


async def shutdown_global_pool_manager():
    """é—œé–‰å…¨åŸŸé€£æ¥æ± ç®¡ç†å™¨"""
    global _global_pool_manager

    if _global_pool_manager:
        await _global_pool_manager.shutdown()
        _global_pool_manager = None


# æ–¹ä¾¿çš„è£é£¾å™¨
def with_connection_pool(session_key: str = "default"):
    """é€£æ¥æ± è£é£¾å™¨"""

    def decorator(func):
        async def wrapper(*args, **kwargs):
            pool_manager = await get_global_pool_manager()
            session = await pool_manager.get_session(session_key)

            # å°‡ session æ³¨å…¥åˆ° kwargs ä¸­
            kwargs["session"] = session
            kwargs["pool_manager"] = pool_manager

            return await func(*args, **kwargs)

        return wrapper

    return decorator
