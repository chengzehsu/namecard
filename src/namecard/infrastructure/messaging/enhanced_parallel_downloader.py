#!/usr/bin/env python3
"""
å¢å¼·ä¸¦è¡Œä¸‹è¼‰å™¨ - EnhancedParallelDownloader
é›†æˆé€£æ¥æ± ç®¡ç†å™¨ï¼Œè§£æ±ºæ‰€æœ‰é€£æ¥æ± ç›¸é—œå•é¡Œ

ä¸»è¦ä¿®å¾©ï¼š
1. ğŸ”§ é€£æ¥æ± æ´©æ¼å•é¡Œå®Œå…¨ä¿®å¾©
2. ğŸ”§ è¶…æ™‚å’Œé‡è©¦æ©Ÿåˆ¶å„ªåŒ–
3. ğŸ”§ ä¸¦ç™¼æ§åˆ¶å’Œè³‡æºç®¡ç†
4. ğŸ”§ éŒ¯èª¤è™•ç†å’Œæ¢å¾©æ©Ÿåˆ¶
5. ğŸ”§ æ€§èƒ½ç›£æ§å’Œè¨ºæ–·
"""

import asyncio
import hashlib
import logging
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import aiofiles
from telegram import File

from .connection_pool_manager import (
    ConnectionPoolConfig,
    ConnectionPoolManager,
    get_global_pool_manager,
)


@dataclass
class EnhancedDownloadResult:
    """å¢å¼·çš„ä¸‹è¼‰çµæœ"""

    success: bool
    data: Optional[bytes] = None
    error: Optional[str] = None
    download_time: float = 0.0
    file_size: int = 0
    source: str = "download"  # download, cache, error
    retry_count: int = 0
    connection_reused: bool = False
    cache_hit: bool = False
    performance_level: str = "A"  # S, A, B, C, D


@dataclass
class DownloadBatch:
    """ä¸‹è¼‰æ‰¹æ¬¡"""

    files: List[Union[File, str]]
    batch_id: str
    max_concurrent: int = 8
    enable_cache: bool = True
    session_key: str = "batch_download"


class EnhancedParallelDownloader:
    """å¢å¼·ä¸¦è¡Œä¸‹è¼‰å™¨ - è§£æ±ºé€£æ¥æ± å•é¡Œ"""

    def __init__(
        self,
        pool_config: Optional[ConnectionPoolConfig] = None,
        enable_cache: bool = True,
        cache_size_mb: int = 200,
        max_concurrent_downloads: int = 8,
    ):
        self.logger = logging.getLogger(__name__)

        # é€£æ¥æ± é…ç½®å„ªåŒ–
        if pool_config is None:
            pool_config = ConnectionPoolConfig(
                total_limit=25,
                per_host_limit=8,
                total_timeout=20,
                connect_timeout=5,
                read_timeout=15,
                max_retries=3,
                cleanup_interval=30,
            )

        self.pool_manager = ConnectionPoolManager(pool_config)
        self.max_concurrent = max_concurrent_downloads

        # å¿«å–é…ç½®
        self.enable_cache = enable_cache
        self.max_cache_size = cache_size_mb * 1024 * 1024
        self.cache_dir = Path("./cache/enhanced_telegram_images")
        if enable_cache:
            self.cache_dir.mkdir(parents=True, exist_ok=True)

        # çµ±è¨ˆè¿½è¹¤ - å¢å¼·ç‰ˆ
        self.stats = {
            "total_downloads": 0,
            "successful_downloads": 0,
            "failed_downloads": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "total_retry_attempts": 0,
            "connection_pool_errors": 0,
            "timeout_errors": 0,
            "avg_download_time": 0.0,
            "total_bytes_downloaded": 0,
            "performance_distribution": {"S": 0, "A": 0, "B": 0, "C": 0, "D": 0},
        }

        # æ€§èƒ½åŸºæº–
        self.performance_thresholds = {
            "S": 0.5,  # è¶…å¿« < 0.5s
            "A": 2.0,  # å¿«é€Ÿ < 2.0s
            "B": 5.0,  # æ­£å¸¸ < 5.0s
            "C": 10.0,  # æ…¢ < 10.0s
            "D": float("inf"),  # å¾ˆæ…¢ >= 10.0s
        }

        self.logger.info("ğŸš€ EnhancedParallelDownloader åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"   - æœ€å¤§ä¸¦ç™¼: {max_concurrent_downloads}")
        self.logger.info(f"   - å¿«å–: {'å•Ÿç”¨' if enable_cache else 'åœç”¨'}")
        self.logger.info(
            f"   - é€£æ¥æ± : ç¸½é™åˆ¶={pool_config.total_limit}, æ¯ä¸»æ©Ÿ={pool_config.per_host_limit}"
        )

    async def initialize(self):
        """åˆå§‹åŒ–é€£æ¥æ± ç®¡ç†å™¨"""
        await self.pool_manager.start_background_cleanup()
        self.logger.debug("âœ… é€£æ¥æ± èƒŒæ™¯æ¸…ç†å·²å•Ÿå‹•")

    def _get_cache_key(self, file_id: str) -> str:
        """ç”Ÿæˆå¿«å–éµ"""
        return hashlib.md5(f"enhanced_{file_id}".encode()).hexdigest()

    def _get_cache_path(self, cache_key: str) -> Path:
        """ç²å–å¿«å–æ–‡ä»¶è·¯å¾‘"""
        return self.cache_dir / f"{cache_key}.jpg"

    async def _check_cache(self, file_id: str) -> Optional[bytes]:
        """æª¢æŸ¥å¿«å– - å¢å¼·ç‰ˆ"""
        if not self.enable_cache:
            self.stats["cache_misses"] += 1
            return None

        cache_key = self._get_cache_key(file_id)
        cache_path = self._get_cache_path(cache_key)

        if cache_path.exists() and cache_path.stat().st_size > 0:
            try:
                async with aiofiles.open(cache_path, "rb") as f:
                    data = await f.read()

                if len(data) > 0:
                    self.stats["cache_hits"] += 1
                    self.logger.debug(
                        f"ğŸ’¾ å¿«å–å‘½ä¸­: {file_id[:12]}... ({len(data)} bytes)"
                    )
                    return data

            except Exception as e:
                self.logger.warning(f"å¿«å–è®€å–å¤±æ•— {file_id[:12]}: {e}")
                # æ¸…ç†æå£çš„å¿«å–
                try:
                    cache_path.unlink()
                except:
                    pass

        self.stats["cache_misses"] += 1
        return None

    async def _store_cache(self, file_id: str, data: bytes) -> bool:
        """å­˜å„²åˆ°å¿«å– - å¢å¼·ç‰ˆ"""
        if not self.enable_cache or len(data) == 0:
            return False

        # æª¢æŸ¥å–®å€‹æ–‡ä»¶å¤§å°é™åˆ¶
        if len(data) > self.max_cache_size // 20:  # å–®æ–‡ä»¶ä¸è¶…éç¸½å¿«å–çš„ 5%
            self.logger.debug(f"æ–‡ä»¶éå¤§ï¼Œè·³éå¿«å–: {len(data)} bytes")
            return False

        try:
            cache_key = self._get_cache_key(file_id)
            cache_path = self._get_cache_path(cache_key)

            # ç¢ºä¿å¿«å–ç›®éŒ„å­˜åœ¨
            cache_path.parent.mkdir(parents=True, exist_ok=True)

            # åŸå­å¯«å…¥
            temp_path = cache_path.with_suffix(".tmp")
            async with aiofiles.open(temp_path, "wb") as f:
                await f.write(data)

            # åŸå­ç§»å‹•
            temp_path.rename(cache_path)

            self.logger.debug(f"ğŸ’¾ å¿«å–å­˜å„²æˆåŠŸ: {file_id[:12]}... ({len(data)} bytes)")

            # ç•°æ­¥æ¸…ç†å¿«å–
            asyncio.create_task(self._cleanup_cache_if_needed())

            return True

        except Exception as e:
            self.logger.warning(f"å¿«å–å­˜å„²å¤±æ•— {file_id[:12]}: {e}")
            return False

    async def _cleanup_cache_if_needed(self):
        """æ™ºèƒ½å¿«å–æ¸…ç†"""
        try:
            total_size = 0
            cache_files = []

            # æƒæå¿«å–ç›®éŒ„
            if not self.cache_dir.exists():
                return

            for file_path in self.cache_dir.iterdir():
                if file_path.is_file() and not file_path.name.endswith(".tmp"):
                    stat = file_path.stat()
                    total_size += stat.st_size
                    cache_files.append((file_path, stat.st_mtime, stat.st_size))

            # æª¢æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†
            if total_size <= self.max_cache_size:
                return

            self.logger.info(
                f"ğŸ“¦ å¿«å–å¤§å°è¶…é™: {total_size / 1024 / 1024:.1f}MB "
                f"(é™åˆ¶: {self.max_cache_size / 1024 / 1024:.1f}MB)"
            )

            # æŒ‰æœ€å¾Œä¿®æ”¹æ™‚é–“æ’åºï¼ˆæœ€èˆŠçš„å…ˆåˆªé™¤ï¼‰
            cache_files.sort(key=lambda x: x[1])

            # åˆªé™¤æª”æ¡ˆç›´åˆ°ä½æ–¼é™åˆ¶çš„ 80%
            target_size = int(self.max_cache_size * 0.8)
            deleted_count = 0
            deleted_size = 0

            for file_path, mtime, size in cache_files:
                if total_size - deleted_size <= target_size:
                    break

                try:
                    file_path.unlink()
                    deleted_count += 1
                    deleted_size += size
                except Exception as e:
                    self.logger.warning(f"åˆªé™¤å¿«å–æ–‡ä»¶å¤±æ•—: {e}")

            if deleted_count > 0:
                self.logger.info(
                    f"ğŸ§¹ å¿«å–æ¸…ç†å®Œæˆ: åˆªé™¤ {deleted_count} å€‹æ–‡ä»¶, "
                    f"é‡‹æ”¾ {deleted_size / 1024 / 1024:.1f}MB"
                )

        except Exception as e:
            self.logger.error(f"å¿«å–æ¸…ç†å¤±æ•—: {e}")

    def _classify_performance(self, download_time: float) -> str:
        """æ€§èƒ½ç­‰ç´šåˆ†é¡"""
        for level, threshold in self.performance_thresholds.items():
            if download_time < threshold:
                return level
        return "D"

    async def download_single_file(
        self, telegram_file: File, session_key: str = "single_download"
    ) -> EnhancedDownloadResult:
        """ä¸‹è¼‰å–®å€‹æ–‡ä»¶ - å®Œå…¨ä¿®å¾©ç‰ˆ"""

        file_id = telegram_file.file_id
        start_time = time.time()

        self.logger.debug(f"ğŸ”½ é–‹å§‹ä¸‹è¼‰: {file_id[:12]}...")

        # 1. æª¢æŸ¥å¿«å–
        cached_data = await self._check_cache(file_id)
        if cached_data:
            download_time = time.time() - start_time
            performance_level = self._classify_performance(download_time)
            self.stats["performance_distribution"][performance_level] += 1

            return EnhancedDownloadResult(
                success=True,
                data=cached_data,
                download_time=download_time,
                file_size=len(cached_data),
                source="cache",
                cache_hit=True,
                performance_level=performance_level,
            )

        # 2. å¯¦éš›ä¸‹è¼‰
        try:
            # ç²å–æ–‡ä»¶ URL
            file_url = telegram_file.file_path
            if not file_url.startswith("http"):
                # æ§‹å»ºå®Œæ•´ URLï¼ˆå¦‚æœéœ€è¦ï¼‰
                file_url = f"https://api.telegram.org/file/bot{telegram_file.file_path}"

            # ä½¿ç”¨é€£æ¥æ± ç®¡ç†å™¨ä¸‹è¼‰
            download_result = await self.pool_manager.download_with_retry(
                url=file_url, session_key=session_key
            )

            self.stats["total_downloads"] += 1

            if download_result["success"]:
                data = download_result["data"]
                download_time = time.time() - start_time
                performance_level = self._classify_performance(download_time)

                # çµ±è¨ˆæ›´æ–°
                self.stats["successful_downloads"] += 1
                self.stats["total_bytes_downloaded"] += len(data)
                self.stats["performance_distribution"][performance_level] += 1
                self.stats["total_retry_attempts"] += (
                    download_result.get("attempt", 1) - 1
                )

                # å­˜å„²åˆ°å¿«å–
                await self._store_cache(file_id, data)

                self.logger.debug(
                    f"âœ… ä¸‹è¼‰å®Œæˆ: {file_id[:12]}... "
                    f"({len(data)} bytes, {download_time:.2f}s, {performance_level}ç´š)"
                )

                return EnhancedDownloadResult(
                    success=True,
                    data=data,
                    download_time=download_time,
                    file_size=len(data),
                    source="download",
                    retry_count=download_result.get("attempt", 1) - 1,
                    performance_level=performance_level,
                )
            else:
                # ä¸‹è¼‰å¤±æ•—
                error_msg = download_result.get("error", "Unknown error")
                download_time = time.time() - start_time

                self.stats["failed_downloads"] += 1

                # éŒ¯èª¤åˆ†é¡çµ±è¨ˆ
                if "timeout" in error_msg.lower():
                    self.stats["timeout_errors"] += 1
                elif "connection" in error_msg.lower() or "pool" in error_msg.lower():
                    self.stats["connection_pool_errors"] += 1

                self.logger.warning(f"âŒ ä¸‹è¼‰å¤±æ•—: {file_id[:12]}... - {error_msg}")

                return EnhancedDownloadResult(
                    success=False,
                    error=error_msg,
                    download_time=download_time,
                    source="error",
                    retry_count=download_result.get("attempt", 1) - 1,
                )

        except Exception as e:
            download_time = time.time() - start_time
            error_msg = f"ä¸‹è¼‰ç•°å¸¸: {str(e)}"

            self.stats["failed_downloads"] += 1
            self.logger.error(f"ğŸ’¥ ä¸‹è¼‰ç•°å¸¸: {file_id[:12]}... - {e}")

            return EnhancedDownloadResult(
                success=False,
                error=error_msg,
                download_time=download_time,
                source="error",
            )

    async def download_batch(
        self, batch: DownloadBatch
    ) -> List[EnhancedDownloadResult]:
        """æ‰¹æ¬¡ä¸‹è¼‰ - å®Œå…¨å„ªåŒ–ç‰ˆ"""

        if not batch.files:
            return []

        batch_start_time = time.time()
        file_count = len(batch.files)

        self.logger.info(
            f"ğŸš€ é–‹å§‹æ‰¹æ¬¡ä¸‹è¼‰: {batch.batch_id} "
            f"({file_count} å€‹æ–‡ä»¶, ä¸¦ç™¼åº¦: {batch.max_concurrent})"
        )

        # ä¸¦ç™¼æ§åˆ¶ä¿¡è™Ÿé‡
        semaphore = asyncio.Semaphore(min(batch.max_concurrent, self.max_concurrent))

        async def download_with_semaphore(
            telegram_file: File, index: int
        ) -> EnhancedDownloadResult:
            """å¸¶ä¿¡è™Ÿé‡æ§åˆ¶çš„ä¸‹è¼‰"""
            async with semaphore:
                try:
                    result = await self.download_single_file(
                        telegram_file,
                        session_key=f"{batch.session_key}_{index % 4}",  # è¼ªè©¢ session
                    )
                    return result
                except Exception as e:
                    self.logger.error(f"æ‰¹æ¬¡ä¸‹è¼‰ç•°å¸¸ #{index}: {e}")
                    return EnhancedDownloadResult(
                        success=False, error=f"æ‰¹æ¬¡ä¸‹è¼‰ç•°å¸¸: {str(e)}", source="error"
                    )

        # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰ä¸‹è¼‰
        try:
            results = await asyncio.gather(
                *[
                    download_with_semaphore(file, i)
                    for i, file in enumerate(batch.files)
                ],
                return_exceptions=True,
            )
        except Exception as e:
            self.logger.error(f"æ‰¹æ¬¡ä¸‹è¼‰ gather å¤±æ•—: {e}")
            # å‰µå»ºå¤±æ•—çµæœ
            results = [
                EnhancedDownloadResult(
                    success=False, error=f"æ‰¹æ¬¡åŸ·è¡Œå¤±æ•—: {str(e)}", source="error"
                )
                for _ in batch.files
            ]

        # è™•ç†çµæœå’Œçµ±è¨ˆ
        processed_results = []
        successful_count = 0
        total_size = 0
        total_cache_hits = 0

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_result = EnhancedDownloadResult(
                    success=False, error=f"è™•ç†ç•°å¸¸: {str(result)}", source="error"
                )
            else:
                processed_result = result

            processed_results.append(processed_result)

            if processed_result.success:
                successful_count += 1
                total_size += processed_result.file_size
                if processed_result.cache_hit:
                    total_cache_hits += 1

        # æ‰¹æ¬¡çµ±è¨ˆ
        batch_duration = time.time() - batch_start_time
        success_rate = successful_count / file_count if file_count > 0 else 0
        avg_time_per_file = batch_duration / file_count if file_count > 0 else 0
        cache_hit_rate = total_cache_hits / file_count if file_count > 0 else 0

        self.logger.info(f"âœ… æ‰¹æ¬¡ä¸‹è¼‰å®Œæˆ: {batch.batch_id}")
        self.logger.info(
            f"   - æˆåŠŸç‡: {successful_count}/{file_count} ({success_rate:.1%})"
        )
        self.logger.info(f"   - ç¸½æ™‚é–“: {batch_duration:.2f}s")
        self.logger.info(f"   - å¹³å‡æ™‚é–“: {avg_time_per_file:.2f}s/æ–‡ä»¶")
        self.logger.info(f"   - å¿«å–å‘½ä¸­ç‡: {cache_hit_rate:.1%}")
        self.logger.info(f"   - ç¸½å¤§å°: {total_size / 1024:.1f}KB")

        return processed_results

    async def get_detailed_stats(self) -> Dict[str, Any]:
        """ç²å–è©³ç´°çµ±è¨ˆä¿¡æ¯"""
        pool_stats = await self.pool_manager.get_connection_stats()

        # è¨ˆç®—å¹³å‡ä¸‹è¼‰æ™‚é–“
        if self.stats["successful_downloads"] > 0:
            self.stats["avg_download_time"] = (
                self.stats["total_bytes_downloaded"]
                / self.stats["successful_downloads"]
                / 1024
            )

        return {
            "downloader_stats": self.stats,
            "connection_pool_stats": pool_stats,
            "cache_stats": {
                "enabled": self.enable_cache,
                "max_size_mb": self.max_cache_size / 1024 / 1024,
                "hit_rate": (
                    self.stats["cache_hits"]
                    / (self.stats["cache_hits"] + self.stats["cache_misses"])
                    if (self.stats["cache_hits"] + self.stats["cache_misses"]) > 0
                    else 0
                ),
            },
            "performance_summary": {
                "success_rate": (
                    self.stats["successful_downloads"] / self.stats["total_downloads"]
                    if self.stats["total_downloads"] > 0
                    else 0
                ),
                "avg_download_time": self.stats["avg_download_time"],
                "performance_distribution": self.stats["performance_distribution"],
            },
        }

    async def diagnose_connection_issues(self) -> Dict[str, Any]:
        """è¨ºæ–·é€£æ¥å•é¡Œ"""
        stats = await self.get_detailed_stats()

        issues = []
        recommendations = []

        # æª¢æŸ¥æˆåŠŸç‡
        success_rate = stats["performance_summary"]["success_rate"]
        if success_rate < 0.9:
            issues.append(f"æˆåŠŸç‡éä½: {success_rate:.1%}")
            recommendations.append("æª¢æŸ¥ç¶²è·¯é€£æ¥å’Œ API é…é¡")

        # æª¢æŸ¥é€£æ¥æ± éŒ¯èª¤
        if self.stats["connection_pool_errors"] > 0:
            issues.append(f"é€£æ¥æ± éŒ¯èª¤: {self.stats['connection_pool_errors']} æ¬¡")
            recommendations.append("å¢åŠ é€£æ¥æ± é™åˆ¶æˆ–æ¸›å°‘ä¸¦ç™¼æ•¸")

        # æª¢æŸ¥è¶…æ™‚éŒ¯èª¤
        if self.stats["timeout_errors"] > 0:
            issues.append(f"è¶…æ™‚éŒ¯èª¤: {self.stats['timeout_errors']} æ¬¡")
            recommendations.append("å¢åŠ è¶…æ™‚æ™‚é–“æˆ–å„ªåŒ–ç¶²è·¯ç’°å¢ƒ")

        # æª¢æŸ¥æ€§èƒ½åˆ†å¸ƒ
        slow_downloads = (
            self.stats["performance_distribution"]["C"]
            + self.stats["performance_distribution"]["D"]
        )
        if slow_downloads > self.stats["total_downloads"] * 0.2:
            issues.append(f"æ…¢é€Ÿä¸‹è¼‰éå¤š: {slow_downloads} æ¬¡")
            recommendations.append("å„ªåŒ–ä¸¦ç™¼è¨­ç½®æˆ–æª¢æŸ¥æœå‹™å™¨æ€§èƒ½")

        # æª¢æŸ¥å¿«å–å‘½ä¸­ç‡
        cache_hit_rate = stats["cache_stats"]["hit_rate"]
        if cache_hit_rate < 0.3 and self.enable_cache:
            issues.append(f"å¿«å–å‘½ä¸­ç‡éä½: {cache_hit_rate:.1%}")
            recommendations.append("å¢åŠ å¿«å–å¤§å°æˆ–å„ªåŒ–å¿«å–ç­–ç•¥")

        return {
            "diagnosis_time": time.time(),
            "health_status": "healthy" if not issues else "issues_detected",
            "issues": issues,
            "recommendations": recommendations,
            "detailed_stats": stats,
        }

    async def cleanup(self):
        """æ¸…ç†è³‡æº"""
        self.logger.info("ğŸ§¹ EnhancedParallelDownloader æ¸…ç†ä¸­...")

        # æ¸…ç†é€£æ¥æ± 
        await self.pool_manager.shutdown()

        self.logger.info("âœ… EnhancedParallelDownloader æ¸…ç†å®Œæˆ")


# å…¨åŸŸå¯¦ä¾‹ç®¡ç†
_global_enhanced_downloader: Optional[EnhancedParallelDownloader] = None


async def get_global_enhanced_downloader() -> EnhancedParallelDownloader:
    """ç²å–å…¨åŸŸå¢å¼·ä¸‹è¼‰å™¨å¯¦ä¾‹"""
    global _global_enhanced_downloader

    if _global_enhanced_downloader is None:
        _global_enhanced_downloader = EnhancedParallelDownloader()
        await _global_enhanced_downloader.initialize()

    return _global_enhanced_downloader


async def shutdown_global_enhanced_downloader():
    """é—œé–‰å…¨åŸŸå¢å¼·ä¸‹è¼‰å™¨"""
    global _global_enhanced_downloader

    if _global_enhanced_downloader:
        await _global_enhanced_downloader.cleanup()
        _global_enhanced_downloader = None
